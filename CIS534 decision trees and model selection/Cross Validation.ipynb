{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c68202f74ce6d38014d0d1026672b01",
     "grade": false,
     "grade_id": "cell-ee3457876c038971",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement cross validation to pick the best `depth` (hyperparameter) for a regression tree, again using the ION dataset.\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b86e22d29a89276fa14001a0ed40bb0d",
     "grade": false,
     "grade_id": "cell-9eed63d0f1650fcc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get Started\n",
    "\n",
    "<p>Let's import a few packages that you will need. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset for this project.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b8d76a6f8d8aeb6844c2211b76847daa",
     "grade": false,
     "grade_id": "cell-39ac94e6e8bdc336",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b8566314b8f92542f0febc4a4bcb41d9",
     "grade": false,
     "grade_id": "cell-e09c08076a310a72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = loadmat(\"ion.mat\")\n",
    "xTr  = data['xTr'].T\n",
    "yTr  = data['yTr'].flatten()\n",
    "xTe  = data['xTe'].T\n",
    "yTe  = data['yTe'].flatten()\n",
    "print(f'Number of features: {xTr.shape[1]}')\n",
    "print(f'Number of training points: {xTr.shape[0]}')\n",
    "print(f'Number of testing points: {xTe.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86d3c886d7cf77aa7e2d194ea37d8193",
     "grade": false,
     "grade_id": "cell-7d95b26aeed38b8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You are provided a regression-tree-based classifier to use. In addition to what you implemented in the first project, we added a `depth` hyperparameter to the classifier, specified as an argument. This `depth` argument allows us to restrict the maximum depth of the tree model. \n",
    "\n",
    "The following code cell shows you how to instantiate a regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92db196a99be53bd1c34810becca3171",
     "grade": false,
     "grade_id": "cell-778baeb6ff2b94be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with no restriction on its depth. \n",
    "# This is equivalent to what you implemented in the previous project\n",
    "\n",
    "# **If you want to create a tree of max depth k, then call RegressionTree(depth=k)**\n",
    "tree = RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92b40d386931b39daf23e0889119b87d",
     "grade": false,
     "grade_id": "cell-37e18ecd068c189d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have also created a square loss function that takes in the prediction <code>pred</code> and ground truth <code>truth</code> and returns the average square loss between prediction and ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2d196ecf3c5c07b95ef72687156f0dc",
     "grade": false,
     "grade_id": "cell-d620b929b0f3b5ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def square_loss(pred, truth):\n",
    "    \"\"\"\n",
    "    Calculates the loss between predicted and true labels.\n",
    "    \n",
    "    Input:\n",
    "        pred: n-dimensional vector of predicted labels\n",
    "        truth: n-dimensional vector of true labels\n",
    "        \n",
    "    Output:\n",
    "        loss: average squared loss\n",
    "    \"\"\"\n",
    "    return np.mean((pred - truth)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7fa4feab4642b2a0fb9ca66ac8eb9150",
     "grade": false,
     "grade_id": "cell-a7b5b10efd4cca7f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, look at the performance of your tree on both the training set and test set using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95f5f7f6610f27c253c3e6b6c7f2afaf",
     "grade": false,
     "grade_id": "cell-bee4f89dde382e5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a9f6dd669f9723a61760d233e63d4a61",
     "grade": false,
     "grade_id": "cell-5c969869a177745f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement Cross Validation\n",
    "\n",
    "As you can see, your tree achives zero training loss on the training set but not zero test loss. Clearly, the tree is overfitting! To reduce overfitting, you need to control the depth of the tree.\n",
    "\n",
    "One way to pick the optimal depth is to do $k$-Fold Cross Validation. To do so, you will first implement `grid_search`, which finds the best depths given a training set and validation set. Then you will implement `generate_kFold`, which generates a pair of training and validation set for grid search. Finally, you will combine the two functions by implementing `cross_validation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb39fccb0124033dfa82e87bda414499",
     "grade": false,
     "grade_id": "cell-88634e1d5f07d0ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part One: Implement `grid_search` [Graded]\n",
    "\n",
    "Implement the function **`grid_search`**, which takes in a training set `xTr, yTr`, a validation set `xVal, yVal` and a list of tree depth candidates `depths`. Your job here is to fit a regression tree for each depth candidate on the training set `xTr, yTr`, evaluate the fitted tree on the validation set `xVal, yVal`, and then pick the candidate that yields the lowest loss for the validation set. \n",
    "\n",
    "**Implementation Notes:**\n",
    "- Use the `square_loss` function to calculate the training and validation loss for corresponding predictions against true labels `yTr` and `yVal` respectively.\n",
    "- In the event of a tie, return the depth that appears first in `depths` list (`np.argmin` on the list of validation losses will give you the first index in case of a tie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2a2a33e56a6faecaab589713820ee7de",
     "grade": false,
     "grade_id": "cell-grid_search",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(xTr, yTr, xVal, yVal, depths):\n",
    "    \"\"\"\n",
    "    Calculates the training and validation loss for trees trained on xTr and validated on yTr with a number of depths.\n",
    "    \n",
    "    Input:\n",
    "        xTr: nxd training data matrix\n",
    "        yTr: n-dimensional vector of training labels\n",
    "        xVal: mxd validation data matrix\n",
    "        yVal: m-dimensional vector of validation labels\n",
    "        depths: a list of len k of depths\n",
    "        \n",
    "    Output:\n",
    "        best_depth, training_losses, validation_losses\n",
    "        best_depth: the depth that yields that lowest validation loss\n",
    "        training_losses: a list of len k. the i-th entry corresponds to the the training loss of the tree of depth=depths[i]\n",
    "        validation_losses: a list of len k. the i-th entry corresponds to the the validation loss of the tree of depth=depths[i]\n",
    "    \"\"\"\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_depth = None\n",
    "    # YOUR CODE HERE\n",
    "    validation_error_star = np.inf\n",
    "    \n",
    "    for i_grid in depths:\n",
    "        # fit model on test data for grid point i in depths\n",
    "        tree_i = RegressionTree(depth=i_grid)\n",
    "        tree_i.fit(xTr, yTr)\n",
    "        # evaluate fit based on training data\n",
    "        training_losses.append(square_loss(tree_i.predict(xTr), yTr))\n",
    "        # evaluate fit based on validation data\n",
    "        validation_error_i = square_loss(tree_i.predict(xVal), yVal)\n",
    "        validation_losses.append(validation_error_i)\n",
    "        if validation_error_i < validation_error_star:\n",
    "            # update error\n",
    "            validation_error_star = validation_error_i\n",
    "            # update depth\n",
    "            best_depth = i_grid\n",
    "    return best_depth, training_losses, validation_losses  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0086e6517fd25f662262b6247c043267",
     "grade": false,
     "grade_id": "cell-ee7f998a81bffac9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can do a dry run with sample depths and plot the training and validation losses against the tree depth. Observe that the training loss continues to decrease with increasing depth, but the validation loss starts to increase after a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "depths = np.arange(10)\n",
    "\n",
    "# 80-20 split of the training data for demo purposes here\n",
    "train_split = int(0.8 * xTr.shape[0])\n",
    "best_depth, training_losses, validation_losses = grid_search(xTr[:train_split], yTr[:train_split], xTr[train_split:], yTr[train_split:], depths)\n",
    "\n",
    "plt.plot(depths, training_losses, label='training')\n",
    "plt.plot(depths, validation_losses, label='validation')\n",
    "plt.xlabel('tree depth')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f'Best depth: {best_depth}, validation loss: {np.min(validation_losses):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "687884caedeec0f1665a5ea0d32e7fa6",
     "grade": false,
     "grade_id": "cell-gridsearch_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of grid search returns the correct number of training and validation loss values and the correct best depth\n",
    "\n",
    "depths = [1,2,3,4,5]\n",
    "k = len(depths)\n",
    "\n",
    "# 80-20 split of the training data\n",
    "train_split = int(0.8 * xTr.shape[0])\n",
    "best_depth, training_losses, validation_losses = grid_search(xTr[:train_split], yTr[:train_split], xTr[train_split:], yTr[train_split:], depths)\n",
    "best_depth_grader, training_losses_grader, validation_losses_grader = grid_search_grader(xTr[:train_split], yTr[:train_split], xTr[train_split:], yTr[train_split:], depths)\n",
    "\n",
    "# Check the length of the training loss\n",
    "def grid_search_test1():\n",
    "    return (len(training_losses) == k) \n",
    "\n",
    "# Check the length of the validation loss\n",
    "def grid_search_test2():\n",
    "    return (len(validation_losses) == k)\n",
    "\n",
    "# Check the argmin\n",
    "def grid_search_test3():\n",
    "    return (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "def grid_search_test4():\n",
    "    return (best_depth == best_depth_grader)\n",
    "\n",
    "def grid_search_test5():\n",
    "    return np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "def grid_search_test6():\n",
    "    return np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "runtest(grid_search_test1, 'grid_search_test1')\n",
    "runtest(grid_search_test2, 'grid_search_test2')\n",
    "runtest(grid_search_test3, 'grid_search_test3')\n",
    "runtest(grid_search_test4, 'grid_search_test4')\n",
    "runtest(grid_search_test5, 'grid_search_test5')\n",
    "runtest(grid_search_test6, 'grid_search_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f9305ab424b43c78b7f8a68549b6ddd",
     "grade": true,
     "grade_id": "cell-gridsearch_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ec449293b4e7cd696516310a70f46a2",
     "grade": true,
     "grade_id": "cell-gridsearch_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5c65fe637a5dabe8cbe213e84494ab60",
     "grade": true,
     "grade_id": "cell-gridsearch_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95401266668db4688e41614590d2efac",
     "grade": true,
     "grade_id": "cell-gridsearch_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18cd7d9e3995e4c8fb94900fa5848d0d",
     "grade": true,
     "grade_id": "cell-gridsearch_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9e6537994a0e1c98cc4f8b5e3f0c47c",
     "grade": true,
     "grade_id": "cell-gridsearch_test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grid search test6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ad3ca325f21e90cf0039136a39d1d6d1",
     "grade": false,
     "grade_id": "cell-67814c28d3b41d3e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Two: Implement `generate_kFold`[Graded]\n",
    "\n",
    "Now, implement the **`generate_kFold`** function, which takes in the number of training examples `n` and the number of folds `k` and returns a list of `k` folds, where each fold takes the form `(training indices, validation indices)`.\n",
    "\n",
    "For instance, if `n = 3` and `k = 3`, then we have three indices `[0, 1, 2]` and we are trying to split it `k = 3` times to obtain different training/validation splits. One _possible_ output of the the function is `[ ([0, 1], [2]), ([1, 2], [0]), ([0, 2], [1]) ]`. It is possible that `k` might not divide `n` with remainder 0. In that case, you can divide `n` `k-1` times fully and have the remainder constitute the final fold. For instance, if `n = 5` and `k = 4`, one _possible_ output is `[ ([1, 2, 3, 4], [0]), ([0, 2, 3, 4], [1]), ([0, 1, 3, 4], [2]), ([0, 1, 2], [3, 4]) ]`.\n",
    "\n",
    "Ensure that no two folds have the same indices as it is wasteful to train a model on the same training/validation split again.\n",
    "\n",
    "One possible algorithm: divide the list of `n` indices into `k` parts and loop `k` times, collecting all but 1 parts into the training set for that fold and leaving that part as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "170ce9825b275c5b5852f449b64c8dd6",
     "grade": false,
     "grade_id": "cell-generate_kFold",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_kFold(n, k):\n",
    "    \"\"\"\n",
    "    Generates [(training_indices, validation_indices), ...] for k-fold validation.\n",
    "    \n",
    "    Input:\n",
    "        n: number of training examples\n",
    "        k: number of folds\n",
    "    \n",
    "    Output:\n",
    "        kfold_indices: a list of length k. Each entry takes the form (training indices, validation indices)\n",
    "    \"\"\"\n",
    "    assert k >= 2\n",
    "    kfold_indices = []\n",
    "    # YOUR CODE HERE\n",
    "    # get fold_size\n",
    "    fold_size = np.floor_divide(n, k)\n",
    "    # initialize indices\n",
    "    indices_n = [*range(n)]\n",
    "    # constructs folds via looping for k-1 folds\n",
    "    for i_fold in range(k-1):\n",
    "        # get validation indices\n",
    "        validation_indices = indices_n[i_fold*fold_size:(fold_size*(1+i_fold))]\n",
    "        # get training indices\n",
    "        training_indices = indices_n.copy()\n",
    "        # remove validation indices\n",
    "        del training_indices[i_fold*fold_size:(fold_size*(1+i_fold))]\n",
    "        # store indices in kfold_indices list\n",
    "        kfold_indices.append((training_indices, validation_indices))\n",
    "    \n",
    "    # add last split manually to take into account rest of data\n",
    "    validation_indices = indices_n[(k-1)*fold_size:]\n",
    "    training_indices = indices_n.copy()\n",
    "    del training_indices[(k-1)*fold_size:]\n",
    "    kfold_indices.append((training_indices, validation_indices))\n",
    "    \n",
    "    return kfold_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'3-fold splits on 3 points: {generate_kFold(3, 3)}')\n",
    "print(f'4-fold splits on 5 points: {generate_kFold(5, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f505a8c3acfdcbe7ee3b2d7312df2a6f",
     "grade": false,
     "grade_id": "cell-generate_Kfold_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your generate_kFold function \n",
    "# returns the correct number of total indices, \n",
    "# train and validation indices, and the correct ratio\n",
    "\n",
    "kfold_indices = generate_kFold(1004, 5)\n",
    "\n",
    "def generate_kFold_test1():\n",
    "    return len(kfold_indices) == 5 # you should generate 5 folds\n",
    "\n",
    "def generate_kFold_test2():\n",
    "    t = [((len(train_indices) + len(validation_indices)) == 1004) \n",
    "         for (train_indices, validation_indices) in kfold_indices]\n",
    "    return np.all(t) # make sure that for each fold, the number of examples sum up to 1004\n",
    "\n",
    "def generate_kFold_test3():\n",
    "    ratio_validation = []\n",
    "    for (train_indices, validation_indices) in kfold_indices:\n",
    "        ratio = len(validation_indices) / len(train_indices)\n",
    "        ratio_validation.append((ratio > 0.24 and ratio < 0.26))\n",
    "    # make sure that for each fold, the training to validation \n",
    "    # examples ratio is in between 0.24 and 0.25\n",
    "    return np.all(ratio_validation) \n",
    "\n",
    "def generate_kFold_test4():\n",
    "    train_indices_set = set() # to keep track of training indices for each fold\n",
    "    validation_indices_set = set() # to keep track of validation indices for each fold\n",
    "    for (train_indices, validation_indices) in kfold_indices:\n",
    "        train_indices_set = train_indices_set.union(set(train_indices))\n",
    "        validation_indices_set = validation_indices_set.union(set(validation_indices))\n",
    "    \n",
    "    # Make sure that you use all the examples in all the training fold and validation fold\n",
    "    return train_indices_set == set(np.arange(1004)) and validation_indices_set == set(np.arange(1004))\n",
    "\n",
    "\n",
    "runtest(generate_kFold_test1, 'generate_kFold_test1')\n",
    "runtest(generate_kFold_test2, 'generate_kFold_test2')\n",
    "runtest(generate_kFold_test3, 'generate_kFold_test3')\n",
    "runtest(generate_kFold_test4, 'generate_kFold_test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "75328718591a0fd8538dac33da76389a",
     "grade": true,
     "grade_id": "cell-generate_kFold_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1170d3b452633348cae8c3ec10aa00f7",
     "grade": true,
     "grade_id": "cell-generate_kFold_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d53da5bdf04c1cd6f110304d673b8d94",
     "grade": true,
     "grade_id": "cell-generate_kFold_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "62986bb58f41cc5b8a3bc4db7b19ca45",
     "grade": true,
     "grade_id": "cell-1b63f3cae2d36064",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs generate Kfold test4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d16b7b6ba4d7c15f8a4fe67674375d6c",
     "grade": false,
     "grade_id": "cell-150c6bb77ba223d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Three: Implement `cross_validation` [Graded]\n",
    "\n",
    "Use `grid_search` to implement the **`cross_validation`** function, which takes in the training set `xTr, yTr`, a list of depth candidates `depths` and performs $k$-Fold Cross Validation on the training set.\n",
    "\n",
    "We will use `generate_kFold` to generate the $k$ training/validation splits and pass in the `indices` for the splits to the `cross_validation` function. Therefore, for each `(training_indices, validation_indices)` element in `indices`, you need to perform grid search to find the training and validation loss for each depth for that fold. Finally, take the average training and validation loss across folds to get the \"average\" loss. Your implementation should return these 2 loss vectors and the depth with the minimum average validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9cc00009d1693d070a8aacfb89078418",
     "grade": false,
     "grade_id": "cell-cross_validation",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(xTr, yTr, depths, indices):\n",
    "    \"\"\"\n",
    "    Performs cross_validation on training data with trees of varying depths. The splits are specified in indices.\n",
    "\n",
    "    Input:\n",
    "        xTr: nxd training data matrix\n",
    "        yTr: n-dimensional vector of training labels\n",
    "        depths: a list of length l of depths to be tried out\n",
    "        indices: indices [(training_indices, validation_indices), ...] from generate_kFold,\n",
    "            specifying the splits for each fold (length k)\n",
    "    \n",
    "    Output:\n",
    "        best_depth: the depth corresponding to the minimum average validation loss.\n",
    "        training_losses: a list of length l. the i-th entry corresponds to the the average training loss of the tree of depth=depths[i]\n",
    "        validation_losses: a list of length l. the i-th entry corresponds to the the average validation loss of the tree of depth=depths[i] \n",
    "    \"\"\"\n",
    "    #training_losses = []\n",
    "    #validation_losses = []\n",
    "    best_depth = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # initialize some matrices\n",
    "    t_loss = np.zeros((len(indices), len(depths)))\n",
    "    v_loss = t_loss.copy()\n",
    "    \n",
    "    # calculate for each grid point and each split the training and validation loss\n",
    "    for i_split, v_split in enumerate(indices):\n",
    "        # get training and validation error for current split\n",
    "        _, training_losses, validation_losses = grid_search(xTr[v_split[0]], \n",
    "                                                            yTr[v_split[0]], \n",
    "                                                            xTr[v_split[1]], \n",
    "                                                            yTr[v_split[1]], depths)\n",
    "        t_loss[i_split, :] = training_losses\n",
    "        v_loss[i_split, :] = validation_losses\n",
    "    \n",
    "    t_loss_avg = np.mean(t_loss, axis=0)\n",
    "    v_loss_avg = np.mean(v_loss, axis=0)\n",
    "    best_depth = depths[np.argmin(v_loss_avg)]\n",
    "    return best_depth, list(t_loss_avg), list(v_loss_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2620fbcab6c252281f46715cf96e050e",
     "grade": false,
     "grade_id": "cell-cross_validation_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of cross_validation returns the correct number of training and validation losses, the correct \"best depth\" and the correct values for training and validation loss\n",
    "\n",
    "depths = [1, 2, 3, 4]\n",
    "k = len(depths)\n",
    "\n",
    "# generate indices\n",
    "# the same indices will be used to cross check your solution and ours\n",
    "indices = generate_kFold(len(xTr), 5)\n",
    "best_depth, training_losses, validation_losses = cross_validation(xTr, yTr, depths, indices)\n",
    "best_depth_grader, training_losses_grader, validation_losses_grader = cross_validation_grader(xTr, yTr, depths, indices)\n",
    "\n",
    "# Check the length of the training loss\n",
    "def cross_validation_test1():\n",
    "    return (len(training_losses) == k) \n",
    "\n",
    "# Check the length of the validation loss\n",
    "def cross_validation_test2():\n",
    "    return (len(validation_losses) == k)\n",
    "\n",
    "# Check the argmin\n",
    "def cross_validation_test3():\n",
    "    return (best_depth == depths[np.argmin(validation_losses)])\n",
    "\n",
    "def cross_validation_test4():\n",
    "    return (best_depth == best_depth_grader)\n",
    "\n",
    "def cross_validation_test5():\n",
    "    return np.linalg.norm(np.array(training_losses) - np.array(training_losses_grader)) < 1e-7\n",
    "\n",
    "def cross_validation_test6():\n",
    "    return np.linalg.norm(np.array(validation_losses) - np.array(validation_losses_grader)) < 1e-7\n",
    "\n",
    "runtest(cross_validation_test1, 'cross_validation_test1')\n",
    "runtest(cross_validation_test2, 'cross_validation_test2')\n",
    "runtest(cross_validation_test3, 'cross_validation_test3')\n",
    "runtest(cross_validation_test4, 'cross_validation_test4')\n",
    "runtest(cross_validation_test5, 'cross_validation_test5')\n",
    "runtest(cross_validation_test6, 'cross_validation_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "364c3cb7a20536a494d8cb6ab52de816",
     "grade": true,
     "grade_id": "cell-cross_validation_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d137eba23f631783a030edfd91eba3d",
     "grade": true,
     "grade_id": "cell-cross_validation_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b673dfaf1264472715790d9c97711c12",
     "grade": true,
     "grade_id": "cell-cross_validation_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45c7fda745aa2eafc3f5adee98488eef",
     "grade": true,
     "grade_id": "cell-cross_validation_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dfe956e764850f0119b012be6ed02b77",
     "grade": true,
     "grade_id": "cell-cross_validation_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b7b103ea48adde5a155329a2abb2312",
     "grade": true,
     "grade_id": "cell-cross_validation_test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs cross validation test6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08020088e73ea149ec22c1cb877cfc15",
     "grade": false,
     "grade_id": "cell-c49b9c1dc00a0424",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Picking the Right Depth Using Cross Validation</h3>\n",
    "\n",
    "Run the following cell and you will see that with the tree with the best depth returned from <code>cross_validation</code> would yield higher training loss but lower test loss (less overfitting)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree = RegressionTree(depth=np.inf)\n",
    "print(\"Learning a tree that can grow to infinite depth\")\n",
    "tree.fit(xTr, yTr)\n",
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))\n",
    "\n",
    "print()\n",
    "print('Cross validate for the best Depth')\n",
    "depths = [1, 3, 5, 7]\n",
    "k = len(depths)\n",
    "indices = generate_kFold(len(xTr), 5)\n",
    "best_depth, training_losses, validation_losses = cross_validation(xTr, yTr, depths, indices)\n",
    "\n",
    "tree = RegressionTree(depth=best_depth)\n",
    "tree.fit(xTr, yTr)\n",
    "print(\"Best Depth\", best_depth)\n",
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b78a9ffab54b7575747f507bce90a82",
     "grade": false,
     "grade_id": "cell-6ef5fcfd300387f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn has an industry-standard implementation for Cross Validation that is extensively used. The usage is also quite straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Learning a tree that can grow to infinite depth\")\n",
    "\n",
    "tree = DecisionTreeRegressor(\n",
    "    criterion='mse', # Impurity function = Mean Squared Error (squared loss)\n",
    "    splitter='best', # Take the best split\n",
    "    max_depth=None, # Expand the tree to the maximum depth possible\n",
    ")\n",
    "tree.fit(xTr, yTr)\n",
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))\n",
    "\n",
    "print()\n",
    "print('Cross validate for the best Depth')\n",
    "depths = [1, 3, 5, 7]\n",
    "\n",
    "# Define grid search with a parameter grid and number of folds\n",
    "tree = GridSearchCV(\n",
    "    DecisionTreeRegressor(), # model\n",
    "    param_grid={\n",
    "        'criterion': ['mse'],\n",
    "        'splitter': ['best'],\n",
    "        'max_depth': depths\n",
    "    }, # grid of parameters\n",
    "    cv=5, # 5 folds\n",
    "    n_jobs=-1, # Run on all available cores\n",
    ")\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "print(\"Best Depth\", tree.best_params_)\n",
    "\n",
    "tree = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=tree.best_params_['max_depth'])\n",
    "tree.fit(xTr, yTr)\n",
    "print('Training Loss: {:.4f}'.format(square_loss(tree.predict(xTr), yTr)))\n",
    "print('Test Loss: {:.4f}'.format(square_loss(tree.predict(xTe), yTe)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}