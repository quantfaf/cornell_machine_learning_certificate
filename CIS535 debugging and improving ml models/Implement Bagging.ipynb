{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c4b1829a0cc90743bb45c3dc99fe92d",
     "grade": false,
     "grade_id": "cell-138bf686eef190d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement bagging for regression trees.\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "74fd3a73b98a01ea45fa31d6f12b5992",
     "grade": false,
     "grade_id": "cell-cfd4728219aa2e6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from helper import *\n",
    "%matplotlib notebook\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1a3d019a7d57fc058d4c25bba5a1573",
     "grade": false,
     "grade_id": "cell-6613495f7b984ddf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this project, we will work with an artificial 2D spiral dataset of size 150 (for visualization), and a high dimensional dataset [ION](https://archive.ics.uci.edu/ml/datasets/Ionosphere) (for a binary test classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9931e62fe1d8f589c5db62eea6e0478b",
     "grade": false,
     "grade_id": "cell-ea9d14b9dc5c69e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "xTrIon, yTrIon, xTeIon, yTeIon = iondata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "print('xTrSpiral shape', xTrSpiral.shape)\n",
    "print('xTrIon shape', xTrIon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33fda24ec71d47ccb39666f0f0ec6d26",
     "grade": false,
     "grade_id": "cell-b74b5db289644016",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will use the regression tree from a previous project. As a reminder, the following code shows you how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a1dce539f074e73e3ae69c8c9da97c2",
     "grade": false,
     "grade_id": "cell-425caeb78952a935",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with no restriction on its depth\n",
    "# and weights for each training example to be 1\n",
    "# if you want to create a tree of max_depth k\n",
    "# then call RegressionTree(depth=k)\n",
    "tree = RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to predict a score for the example\n",
    "score = tree.predict(xTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to make a +1/-1 prediction\n",
    "pred = np.sign(tree.predict(xTrSpiral))\n",
    "        \n",
    "tr_err   = np.mean((np.sign(tree.predict(xTrSpiral)) - yTrSpiral)**2)\n",
    "te_err   = np.mean((np.sign(tree.predict(xTeSpiral)) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc876beefe4ac7feb46d788d3353e7b8",
     "grade": false,
     "grade_id": "cell-a219c78df15f9ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the spiral data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ac6f160c5d69e083239a9cfd5c271851",
     "grade": false,
     "grade_id": "cell-ea356e95d6f8a95d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "                   )\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "tree=RegressionTree(depth=np.inf)\n",
    "tree.fit(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X: tree.predict(X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9fba67b02447472355ac4c0ed15d446",
     "grade": false,
     "grade_id": "cell-d1376bcc54a16465",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bagging in Action\n",
    "\n",
    "### Part One: Implement Bagging [Graded]\n",
    "\n",
    "CART trees are known to be high variance classifiers if trained to full depth. Equivalently, CART trees of full depth easily overfit to the training set, which prevent the trees from performing well on new unseen data. An effective way to prevent overfitting is to use **Bagging** (short for **b**ootstrap **ag**gregating). Implement the function **`forest`**, which builds a forest of `m` regression trees of `depth=maxdepth`. Each tree should be built using training data drawn by randomly sampling `n` examples from the training data with replacement, where `n` is the number of points in `xTr`.\n",
    "\n",
    "We are going to keep it simple and **not** randomly subsample a small set of features to split on. Therefore, all trees will be constructed with a simple call to `RegressionTree(depth=maxdepth)`. The function should output the list of trees.\n",
    "\n",
    "_Hint: You may find [`np.random.choice(a, b, replace=True)`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) useful. It samples `b` numbers from `[0, ..., a-1]` with replacement._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "68f1796b159b1bd5fe08c2941d4a3929",
     "grade": false,
     "grade_id": "cell-forest",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forest(xTr, yTr, m, maxdepth=np.inf):\n",
    "    \"\"\"\n",
    "    Creates a forest of m trees, each of depth=maxdepth.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data points\n",
    "        yTr:      n-dimensional vector of labels\n",
    "        m:        number of trees in the forest\n",
    "        maxdepth: maximum depth of each tree\n",
    "        \n",
    "    Output:\n",
    "        trees: list of decision trees of length m\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    trees = []\n",
    "    # YOUR CODE HERE\n",
    "    # sample m data sets\n",
    "    indices = np.arange(n)\n",
    "    for i_m in range(m):  \n",
    "        # get data set i_m\n",
    "        indices_i_m = np.random.choice(indices, n, replace=True)\n",
    "        xTr_i_m = xTr[indices_i_m]\n",
    "        yTr_i_m = yTr[indices_i_m]\n",
    "        # train for each resampled data set a full decision tree\n",
    "        tree = RegressionTree(depth=maxdepth)\n",
    "        # train and fit\n",
    "        tree.fit(xTr_i_m, yTr_i_m)\n",
    "        # append list\n",
    "        trees.append(tree)\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1b497bc9d184d6c1158c0b69aad1b53e",
     "grade": false,
     "grade_id": "cell-forest-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forest_test1():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "    return len(trees) == m # make sure there are m trees in the forest\n",
    "\n",
    "def forest_test2():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    max_depth = 4\n",
    "    trees = forest(x, y, m, max_depth)\n",
    "    depths_forest = np.array([tree.depth for tree in trees]) # Get the depth of all trees in the forest\n",
    "    return np.all(depths_forest == max_depth) # make sure that the max depth of all the trees is correct\n",
    "\n",
    "\n",
    "def forest_test3():\n",
    "    s = set()\n",
    "\n",
    "    def DFScollect(tree):\n",
    "        # Do Depth first search to all prediction in the tree\n",
    "        if tree.left is None and tree.right is None:\n",
    "            s.add(tree.prediction)\n",
    "        else:\n",
    "            DFScollect(tree.right)\n",
    "            DFScollect(tree.left)\n",
    "\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m);\n",
    "\n",
    "    lens = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        s.clear()\n",
    "        DFScollect(trees[i].root)\n",
    "        lens[i] = len(s)\n",
    "\n",
    "    # Check that about 63% of data is represented in each random sample\n",
    "    return abs(np.mean(lens) - 100 * (1 - 1 / np.exp(1))) < 2\n",
    "\n",
    "runtest(forest_test1, 'forest_test1')\n",
    "runtest(forest_test2, 'forest_test2')\n",
    "runtest(forest_test3, 'forest_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54fa70ea09af0a38ae6db2820a018b9c",
     "grade": true,
     "grade_id": "cell-forest-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59f4cca5ae2d595dd05629d0269700a3",
     "grade": true,
     "grade_id": "cell-forest-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18c95683f1b767648bc61e8ba41a6282",
     "grade": true,
     "grade_id": "cell-forest-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs forest_test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a604c40372b8aeb729d635588c9537ca",
     "grade": false,
     "grade_id": "cell-0d951c782f2e7d46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Two: Implement `evalforest` [Graded]\n",
    "\n",
    "Now implement the function **`evalforest`**, which should take as input a set of $m$ trees and a set of $n$ test inputs and return the **average** prediction of all the trees.\n",
    "\n",
    "Note that for bagging, we take the average over all trees weighted equally. In a later project, you will implement a different version of this function that assigns different weights to predictions of different trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "60e705738054dcef05183252a6790152",
     "grade": false,
     "grade_id": "cell-evalforest",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evalforest(trees, X):\n",
    "    \"\"\"\n",
    "    Evaluates X using trees.\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of length m of RegressionTree decision trees\n",
    "        X:      n x d matrix of data points\n",
    "        \n",
    "    Output:\n",
    "        pred:   n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    \n",
    "    pred = np.zeros(n)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    pred_sum = np.zeros(n)\n",
    "    # loop over trees and calculate prediction\n",
    "    for i_tree, v_tree in enumerate(trees):\n",
    "        i_predict = v_tree.predict(X)\n",
    "        pred_sum += i_predict\n",
    "    pred = (1/m) * pred_sum\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e3521d1a3b47b82dd4f97951fa58bab",
     "grade": false,
     "grade_id": "cell-evalforest-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evalforest_test1():\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m)\n",
    "    \n",
    "    preds = evalforest(trees, x)\n",
    "    return preds.shape == y.shape\n",
    "\n",
    "def evalforest_test2():\n",
    "    m = 200\n",
    "    x = np.ones(10).reshape((10, 1))\n",
    "    y = np.ones(10)\n",
    "    max_depth = 0\n",
    "    \n",
    "    # Create a forest with trees depth 0\n",
    "    # Since the data are all ones, each tree will return 1 as prediction\n",
    "    trees = forest(x, y, m, 0) \n",
    "    pred = evalforest(trees, np.ones((1, 1)))[0]\n",
    "    return np.isclose(pred,1) # the prediction should be equal to the sum of weights\n",
    "    \n",
    "def bagging_test1():\n",
    "    m = 50\n",
    "    xTr = np.random.rand(500,3) - 0.5\n",
    "    yTr = np.sign(xTr[:,0] * xTr[:,1] * xTr[:,2]) # XOR Classification\n",
    "    xTe = np.random.rand(50,3) - 0.5\n",
    "    yTe = np.sign(xTe[:,0] * xTe[:,1] * xTe[:,2])\n",
    "\n",
    "    tree = RegressionTree(depth=5)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneacc = np.sum(np.sign(tree.predict(xTe)) == yTe)\n",
    "\n",
    "    trees = forest(xTr, yTr, m, maxdepth=5)\n",
    "    multiacc = np.sum(np.sign(evalforest(trees, xTe)) == yTe)\n",
    "\n",
    "    # Check that bagging yields improvement - or doesn't get too much worse\n",
    "    return multiacc * 1.1 >= oneacc\n",
    "\n",
    "def bagging_test2():\n",
    "    m = 50\n",
    "    xTr = (np.random.rand(500,3) - 0.5) * 4\n",
    "    yTr = xTr[:,0] * xTr[:,1] * xTr[:,2] # XOR Regression\n",
    "    xTe = (np.random.rand(50,3) - 0.5) * 4\n",
    "    yTe = xTe[:,0] * xTe[:,1] * xTe[:,2]\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    tree = RegressionTree(depth=3)\n",
    "    tree.fit(xTr, yTr)\n",
    "    oneerr = np.sum(np.sqrt((tree.predict(xTe) - yTe) ** 2))\n",
    "\n",
    "    trees = forest(xTr, yTr, m, maxdepth=3)\n",
    "    multierr = np.sum(np.sqrt((evalforest(trees, xTe) - yTe) ** 2))\n",
    "\n",
    "    # Check that bagging yields improvement - or doesn't get too much worse\n",
    "    return multierr <= oneerr * 1.5\n",
    "\n",
    "runtest(evalforest_test1, 'evalforest_test1')\n",
    "runtest(evalforest_test2, 'evalforest_test2')\n",
    "runtest(bagging_test1, 'bagging_test1')\n",
    "runtest(bagging_test2, 'bagging_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "019884dcf97cdccb16ae2cfe5b83ddce",
     "grade": true,
     "grade_id": "cell-evalforest-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalforest-test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "94aa410d89836b20ec61a23c89b833f7",
     "grade": true,
     "grade_id": "cell-evalforest-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalforest-test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3dad43cc00473e419d493db7aff6e31",
     "grade": true,
     "grade_id": "cell-bagging-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs bagging-test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a1d0f1908ac4204b5ab6452a5bc695bc",
     "grade": true,
     "grade_id": "cell-bagging-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs bagging-test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92aa00b78eb8e51c60efafa1e6ec4489",
     "grade": false,
     "grade_id": "cell-a84a7ba4a0e5d391",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize the Decision Boundary\n",
    "\n",
    "The following script visualizes the decision boundary of an ensemble of decision tree. You might observe that the decision boundary is less rigid with an ensemble of 50 trees than with just 1 CART tree. This is to be expected as a forest is just an ensemble of CART trees and averages the predictions. Consequently, the test error should also be less with the the ensemble than with just 1 CART tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46170c5267f1c5475b50d88ec7f2de4e",
     "grade": false,
     "grade_id": "cell-0b793c46d539ba5d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trees=forest(xTrSpiral,yTrSpiral, 50) # compute tree on training data \n",
    "visclassifier(lambda X:evalforest(trees,X),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2d4d2803adef1cc39021a120ca054272",
     "grade": false,
     "grade_id": "cell-03a78b7b9ea09b9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Evaluate Test and Training Error</h3>\n",
    "\n",
    "<p>The following script evaluates the test and training error of an ensemble of decision trees as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "296fcdfb107ebeb329dfa6808da2f8a1",
     "grade": false,
     "grade_id": "cell-4c5550a5997aaf04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "M=20 # max number of trees\n",
    "err_trB=[]\n",
    "err_teB=[]\n",
    "alltrees=forest(xTrIon,yTrIon,M)\n",
    "for i in range(M):\n",
    "    trees=alltrees[:i+1]\n",
    "    trErr = np.mean(np.sign(evalforest(trees,xTrIon)) != yTrIon)\n",
    "    teErr = np.mean(np.sign(evalforest(trees,xTeIon)) != yTeIon)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    print(\"[%d]training err = %.4f\\ttesting err = %.4f\" % (i,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M), err_trB, '-*', label=\"Training error\")\n",
    "line_te, = plt.plot(range(M), err_teB, '-*', label=\"Testing error\")\n",
    "plt.title(\"Ensemble of Decision Trees\")\n",
    "plt.legend(handles=[line_tr, line_te])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1D Interactive Demo\n",
    "\n",
    "The next interactive demo shows a 1-dimensional curve fitted with ensembles of decision trees. We sample 100 training data points from the curve with additive noise. Note how the predicted curve becomes increasingly smooth as you add trees &mdash; adding trees should thus decrease training and testing error. The testing error may be a little higher than the training error, but there is no large gap as ensembles are quite good at avoiding overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,Q,trees\n",
    "    \n",
    "    if event.key == 'shift': Q+=10\n",
    "    else: Q+=1\n",
    "    Q=min(Q,M)\n",
    "\n",
    "\n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    pTest=evalforest(trees[:Q],xTest);\n",
    "    pTrain=evalforest(trees[:Q],xTrain);\n",
    "\n",
    "\n",
    "    errTrain=np.sqrt(np.mean((pTrain-yTrain)**2))\n",
    "    errTest=np.sqrt(np.mean((pTest-yTest)**2))\n",
    "\n",
    "    plt.plot(xTrain[:,0],yTrain,'bx')\n",
    "    plt.plot(xTest[:,0],pTest,'r-')\n",
    "    plt.plot(xTest[:,0],yTest,'k-')\n",
    "\n",
    "    plt.legend(['Training data','Prediction'])\n",
    "    plt.title('(%i Trees)  Error Tr: %2.4f, Te:%2.4f' % (Q,errTrain,errTest))\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "n=100; # number of training points\n",
    "NOISE=0.05 # magnitude of noise\n",
    "xTrain=np.array([np.linspace(0,1,n),np.zeros(n)]).T\n",
    "yTrain=2*np.sin(xTrain[:,0]*3)*(xTrain[:,0]**2)\n",
    "yTrain+=np.random.randn(yTrain.size)*NOISE;\n",
    "ntest=300; # density of test points\n",
    "xTest=np.array([linspace(0,1,ntest),np.zeros(ntest)]).T\n",
    "yTest=2*np.sin(xTest[:,0]*3)*(xTest[:,0]**2)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Hyper-parameters (feel free to play with them)\n",
    "M=100 # number of trees\n",
    "depth=np.inf\n",
    "trees=forest(xTrain, yTrain, M,maxdepth=depth)\n",
    "Q=0;\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest) \n",
    "print('Click to add a tree (shift-click and add 10 trees).');\n",
    "plt.title('Click to start boosting on this 1D data (Shift-click to add 10 trees).')\n",
    "plt.plot(xTrain[:,0],yTrain,'*')\n",
    "plt.plot(xTest[:,0],yTest,'k-')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>2D Interactive Demo</h3>\n",
    "\n",
    "The following demo visualizes the bagged classifier. Add your own points directly on the graph with click and shift+click to see the prediction boundaries. There will be a delay between clicks as the new classifier is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "58f25174e5505da24018a128518a020f",
     "grade": false,
     "grade_id": "cell-614060398e2f62ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,w,b,M\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTrain = np.concatenate((xTrain,pos), axis = 0)\n",
    "    yTrain = np.append(yTrain, label)\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(yTrain)\n",
    "        \n",
    "    w = np.array(w).flatten()\n",
    "    \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get forest\n",
    "    trees=forest(xTrain,yTrain,M)\n",
    "    fun = lambda X:evalforest(trees,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],\n",
    "            xTrain[yTrain == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "xTrain= np.array([[5,6]])\n",
    "b=yTrIon\n",
    "yTrain = np.array([1])\n",
    "w=xTrIon\n",
    "M=20\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest)\n",
    "print('Note: You may notice a delay when adding points to the visualization.')\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}