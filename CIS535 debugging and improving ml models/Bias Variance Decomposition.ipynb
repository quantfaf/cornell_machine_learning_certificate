{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "50abbe56399b3c9ca56f1be4da6136c7",
     "grade": false,
     "grade_id": "cell-1e2b137b9f48bf8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "In this project, you will decompose the test loss into the consituent components: bias, variance, and noise, and analyze their behavior.\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c67c5a4c4c0b156fef91e5f7258be8f2",
     "grade": false,
     "grade_id": "cell-e5ced222ffb03a72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73e1a6b0ae2ac423232bbf44026ac693",
     "grade": false,
     "grade_id": "cell-53fdcf615afb6769",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bias-Variance Decomposition\n",
    "\n",
    "Recall that the generalization error of squared loss can be decomposed into bias, variance, and noise:\n",
    "$$\n",
    "\\underbrace{\\mathbb{E} \\left[ \\left( h_D(\\mathbf{x}) - y \\right)^2 \\right]}_\\text{Generalization Error} = \\underbrace{\\mathbb{E} \\left[ \\left( h_D (\\mathbf{x}) - \\overline{h} (\\mathbf{x}) \\right)^2 \\right]}_\\text{Variance} + \\underbrace{\\mathbb{E} \\left[ \\left( \\overline{h}(\\mathbf{x}) - \\overline{y}(\\mathbf{x}) \\right)^2 \\right]}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E} \\left[ \\left( \\overline{y}(\\mathbf{x}) - y(\\mathbf{x}) \\right)^2 \\right]}_\\mathrm{Noise}\n",
    "$$\n",
    "    \n",
    "We will now create a data set for which we can approximately compute this decomposition. Since we cannot calculate the Generalization Error, we will estimate it using the loss on the test set. The function **`toydata`** generates a 2-dimensional binary data set (classes 1 and 2). Both are sampled from Gaussian distributions:\n",
    "$$\n",
    "P \\left( \\mathbf{x} \\; | \\; y=1 \\right) \\sim \\mathcal{N}( \\mathbf{0}, I) \\text{ and } P \\left( \\mathbf{x} \\; | \\; y=2 \\right) \\sim \\mathcal{N} \\left( \\mathbf{\\mu}, I \\right)\n",
    "$$\n",
    "where $\\mathbf{\\mu} = \\begin{bmatrix}1.75\\\\1.75\\end{bmatrix}$. In code, we will use the global variable `OFFSET = 1.75` to represent $\\mathbf{\\mu}$.\n",
    "\n",
    "### Computing noise, bias and variance\n",
    "\n",
    "You will implement five functions: **`computeybar`**, **`computenoise`**, **`computehbar`**, **`computebias`**, and **`computevariance`**. The functions `computeybar` and `computehbar` are helper functions to calculate the three components of squared error.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`toydata` is a helper function that generates the $n$ 2-dimensional data points with $n/2$ values in class 1 and $n/2$ values in class 2.\n",
    "- Class 1 is the label for data drawn from a normal distribution with mean $\\mathbf{0}$ and standard deviation $\\sigma = I$. That is, first dimension is drawn with mean $0$ and std. dev. $1$, and second dimension is independently drawn with mean $0$ and std. dev. $1$.\n",
    "- Class 2 is the label for data drawn from a normal distribution with mean $\\mathbf{\\mu}$ and $\\sigma = I$. That is, first dimension is drawn with mean $1.75$ and std. dev. $1$, and second dimension is independelty drawn with mean $1.75$ and std. dev. $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "821fb8615c196b1bf4b0689a6bd001c4",
     "grade": false,
     "grade_id": "cell-68beb9e0fa121c43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OFFSET = 1.75 # mu = [OFFSET, OFFSET]\n",
    "X, y = toydata(OFFSET, 1000)\n",
    "\n",
    "# Visualize the generated data\n",
    "ind1 = y == 1\n",
    "ind2 = y == 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[ind1, 0], X[ind1, 1], c='r', marker='o', label='Class 1')\n",
    "plt.scatter(X[ind2, 0], X[ind2, 1], c='b', marker='o', label='Class 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f61845fdd9c76df44b2c018213120b6",
     "grade": false,
     "grade_id": "cell-d4332b3b2f191280",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part One: Noise [Graded]\n",
    "You will first implement functions to compute the noise. For this, you need to implement **`computeybar`** to compute the expected label for a given input $\\mathbf{x}$:\n",
    "$$\n",
    "\\overline{y}(\\mathbf{x}) = 1 \\cdot P \\left( y=1 \\; | \\; \\mathbf{x} \\right) + 2 \\cdot P \\left( y=2 \\; | \\; \\mathbf{x} \\right)\n",
    "$$\n",
    "\n",
    "You can compute the probability $P \\left( \\mathbf{x} \\; | \\; y = c \\right)$ with the equations $P \\left( \\mathbf{x} \\; | \\; y=1 \\right) \\sim \\mathcal{N}(\\mathbf{0}, I) \\text{ and } P \\left( \\mathbf{x} \\; | \\; y=2 \\right) \\sim \\mathcal{N} ( \\mathbf{\\mu}, I)$. Then use Bayes rule to compute\n",
    "\n",
    "$$\n",
    "P \\left( y=c \\; | \\; \\mathbf{x} \\right) = \\frac{ P \\left( \\mathbf{x} \\; | \\; y=c \\right) \\cdot P(y=c) }{ P \\left( \\mathbf{x} \\; | \\; y=1 \\right) \\cdot P(y=1) + P \\left( \\mathbf{x} \\; | \\; y=2 \\right) \\cdot P(y=2)}\n",
    "$$\n",
    "\n",
    "**Implementation Notes:**\n",
    "- Use the function `normpdf` to compute $P \\left( [\\mathbf{x}]_\\alpha \\; | \\; y = c \\right)$, which is defined for you inside `computeybar`. Note that `normpdf` only computes $P \\left( [\\mathbf{x}]_\\alpha \\; | \\; y = c \\right)$ in a single dimension $\\alpha$, but you can use the fact that both dimensions are independent to obtain, for example, $P \\left( \\mathbf{x} \\; | \\; y=1 \\right) = P \\left( [\\mathbf{x}]_1 \\; | \\; y=1 \\right) \\cdot P \\left( [\\mathbf{x}]_2 \\; | \\; y=1 \\right)$.\n",
    "- Since `toyData` generated $n/2$ points for both classes, $P(y = c) = 1/2$ for $c = 1, 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "049b26398cbb1dc2056e8eb8a20510d6",
     "grade": false,
     "grade_id": "cell-computeybar",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def computeybar(xTe, OFFSET):\n",
    "    \"\"\"\n",
    "    Computes the expected label 'ybar' for a set of inputs xTe, \n",
    "    generated from two Normal distributions (the first with mean 0 and std. dev. I\n",
    "    and the second with mean [OFFSET, OFFSET] and std. dev. I).\n",
    "    \n",
    "    Input:\n",
    "        xTe    : data matrix of shape nx2\n",
    "        OFFSET : The OFFSET passed into the toyData function. The difference in the\n",
    "                 mu of normal distributions for points with labels class1 and class2.\n",
    "\n",
    "    Output:\n",
    "        ybar   : a nx1 vector of the expected labels for each vector in xTe\n",
    "    \"\"\"\n",
    "    n, d = xTe.shape\n",
    "    ybar = np.zeros(n)\n",
    "    \n",
    "    # Feel free to use the following function to compute P( [x]_\\alpha | y )\n",
    "    normpdf = lambda x, mu, sigma: np.exp(-0.5 * np.power((x - mu) / sigma, 2)) / (np.sqrt(2 * np.pi) * sigma)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # caculate y bar\n",
    "    pr_y1 = 0.5\n",
    "    pr_y2 = 1-pr_y1\n",
    "    # class 1: P(x|y=c1ass1)\n",
    "    pr_x_y1 = normpdf(xTe, 0, 1)\n",
    "    pr_xbold_y1 = np.multiply(pr_x_y1[:, 0], pr_x_y1[:, 1])\n",
    "    # class 2: P(x|y=c1ass2)\n",
    "    pr_x_y2 = normpdf(xTe, OFFSET, 1)\n",
    "    pr_xbold_y2 = np.multiply(pr_x_y2[:, 0], pr_x_y2[:, 1])\n",
    "    # apply Bayes rule\n",
    "    pr_y1_xbold = (pr_xbold_y1 * pr_y1) / (pr_xbold_y1 * pr_y1 + pr_xbold_y2 * pr_y2)\n",
    "    pr_y2_xbold = (pr_xbold_y2 * pr_y2) / (pr_xbold_y1 * pr_y1 + pr_xbold_y2 * pr_y2)\n",
    "    # get ybar\n",
    "    ybar = 1.0 * pr_y1_xbold + 2.0 * pr_y2_xbold\n",
    "    return ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e25306cc611106615ef953d6e91bbe51",
     "grade": false,
     "grade_id": "cell-ybar-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_ybar1():\n",
    "    OFFSET = 2\n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n) # Generate n datapoints\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    \n",
    "    return ybar.shape == (n, ) # the output of your ybar should be a n dimensional array\n",
    "\n",
    "def test_ybar2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [ 51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    return np.isclose(np.mean(np.power(yTe - ybar, 2)), 0)\n",
    "\n",
    "def test_ybar3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    \n",
    "    return np.mean(np.power(yTe - ybar, 2)) < 0.0002 # make sure the noise is small\n",
    "\n",
    "runtest(test_ybar1, 'test_ybar1')\n",
    "runtest(test_ybar2, 'test_ybar2')\n",
    "runtest(test_ybar3, 'test_ybar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "131db4a29206f408e8a0ae70fd339978",
     "grade": true,
     "grade_id": "cell-ybar1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "353c28393e8c908ab31cb74cf0e20ede",
     "grade": true,
     "grade_id": "cell-ybar2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19f1fd05e5b1daaf31f4190d914556c1",
     "grade": true,
     "grade_id": "cell-ybar3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_ybar3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7496606ff4fabc23262ecb2b611c388",
     "grade": false,
     "grade_id": "cell-f2ed3ee42a95b912",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Two: Noise continued [Graded]\n",
    "\n",
    "Now, calculate the noise component $\\mathbb{E} \\left[ \\left( \\overline{y}(\\mathbf{x}) - y(\\mathbf{x}) \\right)^2 \\right]$ of the error using `computeybar` that you implemented above. Remember that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( \\overline{y}(\\mathbf{x}) - y(\\mathbf{x}) \\right)^2 \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n \\left( \\overline{y} \\left( \\mathbf{x}_i \\right) - y \\left( \\mathbf{x}_i \\right) \\right)^2\n",
    "$$\n",
    "\n",
    "and that `computeybar` computes $\\overline{y}(\\mathbf{x}) = \\left[ \\overline{y} \\left( \\mathbf{x}_1 \\right), \\dots, \\overline{y} \\left( \\mathbf{x}_n \\right) \\right]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f44c1b0d456df36555ad874809254df7",
     "grade": false,
     "grade_id": "cell-computenoise",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def computenoise(xTe, yTe, OFFSET):\n",
    "    \"\"\"\n",
    "    Computes the noise, or square mean of ybar - y, for a set of inputs (xTe, yTe) generated with toyData\n",
    "    using OFFSET.\n",
    "\n",
    "    Input:\n",
    "        xTe       : data matrix of shape nx2\n",
    "        yTe       : n-dimensional array of true labels\n",
    "        OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                    mu of normal distributions for points with labels class1 and class2.\n",
    "\n",
    "    Output:\n",
    "        noise:    : a scalar representing the noise component of the error of xTe\n",
    "    \"\"\"\n",
    "    noise = 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # calculate noise\n",
    "    n = yTe.size\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = (1/n) * np.sum(np.square(ybar - yTe))\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "13e22da5eeafaa2e8e1e03f488124888",
     "grade": false,
     "grade_id": "cell-computenoise-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_noise1():\n",
    "    OFFSET = 2\n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n) # Generate n datapoints\n",
    "    noise = computenoise(xTe, yTe, OFFSET)\n",
    "    \n",
    "    return np.isscalar(noise) \n",
    "\n",
    "def test_noise2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [ 51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    noise = computenoise(xTe, yTe, OFFSET)\n",
    "    return np.isclose(noise,0)\n",
    "\n",
    "def test_noise3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = computenoise(xTe,yTe,OFFSET)\n",
    "    \n",
    "    return noise < 0.0002 # make sure the noise is small\n",
    "\n",
    "runtest(test_noise1, 'test_noise1')\n",
    "runtest(test_noise2, 'test_noise2')\n",
    "runtest(test_noise3, 'test_noise3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8b496f846479473836d453dff0c0214",
     "grade": true,
     "grade_id": "cell-test_noise1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_noise1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "49f9155799a7d7d9e490eaf9ac64bf7f",
     "grade": true,
     "grade_id": "cell-test_noise2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_noise2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "361a4d8fc100e6c0bf27d59c9653e81d",
     "grade": true,
     "grade_id": "cell-test_noise3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_noise3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8663d0291561de1ab0c202e18f64dc21",
     "grade": false,
     "grade_id": "cell-2a0090c31f38a547",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Visualizing the Data</h3>\n",
    "<p>You can now see the error of the bayes classifier. Below is a plot of the two classes of points and the misclassified points.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2987b720bd68e675607ef55b45ecfaf6",
     "grade": false,
     "grade_id": "cell-ad17a864ee4ad0e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OFFSET = 1.75\n",
    "np.random.seed(1)\n",
    "xTe, yTe = toydata(OFFSET, 1000)\n",
    "\n",
    "# compute Bayes Error\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "predictions = np.round(ybar)\n",
    "errors = predictions != yTe\n",
    "err = errors.sum() / len(yTe) * 100\n",
    "print('Error of Bayes classifier: %.2f%%.' % err)\n",
    "\n",
    "# print out the noise\n",
    "print('Noise: %.4f' % computenoise(xTe, yTe, OFFSET))\n",
    "\n",
    "# plot data\n",
    "ind1 = yTe == 1\n",
    "ind2 = yTe == 2\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xTe[ind1, 0], xTe[ind1, 1], c='r', marker='o')\n",
    "plt.scatter(xTe[ind2, 0], xTe[ind2, 1], c='b', marker='o')\n",
    "plt.scatter(xTe[errors, 0], xTe[errors, 1], c='k', s=100, alpha=0.25)\n",
    "plt.title(\"Plot of data (misclassified points highlighted with a black circle)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2957c1a5ff66fc579b08530c4eb56506",
     "grade": false,
     "grade_id": "cell-0f052191b9c2739c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see from the graph above, the dataset is noisy and cannot be classified into two groups cleanly, thus giving non-zero noise and non-zero bayes error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fc28513b22b3be5b440c8e3f217492a",
     "grade": false,
     "grade_id": "cell-f05e0af9fb83da3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Three: Bias [Graded]\n",
    "\n",
    "For the bias, you will need the average classifier $\\overline{h}$. Although you cannot compute the expected value $\\overline{h} = \\mathbb{E}[h]$, you can approximate it by sampling many training sets $D_1, \\dots, D_m$, and training a classifier on each to get $h_{D_1}, \\dots, h_{D_m}$. You can then average their predictions on each data point:\n",
    "\n",
    "$$\n",
    "\\overline{h}(\\mathbf{x}) \\approx \\frac{1}{m} \\sum_{j=1}^m h_{D_j}(\\mathbf{x})\n",
    "$$\n",
    "    \n",
    "Edit the function **`computehbar`** to do this. You should average over `NMODELS` ($m$) different $h_{D_j}$, each trained on a different data set of `Nsmall` ($n$) inputs drawn from the same distribution. You essentially need to create `NMODELS` toy datasets of size `Nsmall` using `toydata(OFFSET, Nsmall)` and evaluate the trained classifier on `xTe`.\n",
    "\n",
    "We are going to use regression trees as our $h_{D_j}$ . The following code shows how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5930f6146191f5ab3b1a46a3dee19eef",
     "grade": false,
     "grade_id": "cell-660c9a79c812ba96",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xTr, yTr = toydata(OFFSET, 100)\n",
    "\n",
    "# Create a regression tree with no restriction on its depth\n",
    "# if you want to create a tree of depth k\n",
    "# then call RegressionTree(depth=k)\n",
    "tree = RegressionTree(depth=np.inf)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTr, yTr)\n",
    "\n",
    "# To use the trained regression tree to make prediction\n",
    "pred = tree.predict(xTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae5e9fbf7cac5b3e38f69d818fa3143f",
     "grade": false,
     "grade_id": "cell-computehbar",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def computehbar(xTe, depth, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    Computes the prediction of the average regression tree (hbar) on dataset xTe.\n",
    "    Each of the NMODELS regression trees used in the average tree should be\n",
    "    trained using data from toydata(OFFSET, Nsmall).\n",
    "\n",
    "    Input:\n",
    "        xTe       : data matrix of shape nx2\n",
    "        depth     : Depth of each regression tree to be trained\n",
    "        Nsmall    : Number of points in the dataset that each tree is trained on\n",
    "        NMODELS   : Number of regression trees to train\n",
    "        OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                    mu of normal distributions for points with labels class1 and class2.\n",
    "    Output:\n",
    "        hbar      : a nx1 vector of the expected labels for each vector in xTe\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    hbar = np.zeros(n)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for i_model in range(NMODELS):\n",
    "        # get toy data\n",
    "        i_xTr, i_yTr = toydata(OFFSET, Nsmall)\n",
    "        # create decision tree\n",
    "        tree = RegressionTree(depth=depth)\n",
    "        # fit tree on sample i\n",
    "        tree.fit(i_xTr, i_yTr)\n",
    "        # predict on test\n",
    "        pred = tree.predict(xTe)\n",
    "        hbar += pred\n",
    "    \n",
    "    return (1/NMODELS) * hbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19e00fb8c50820a44c076f2e4785c309",
     "grade": false,
     "grade_id": "cell-hbar-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_hbar1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return hbar.shape == (n, ) # the dimension of hbar should be (n, )\n",
    "\n",
    "def test_hbar2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 1\n",
    "    \n",
    "    # since the mean is far apart, the tree should be able to learn perfectly\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = computeybar_grader(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar,2))\n",
    "    return np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "def test_hbar3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    np.random.seed(1)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    ybar = computeybar_grader(xTe, OFFSET)\n",
    "    bias = np.mean(np.power(hbar-ybar, 2))\n",
    "    return np.abs(bias - 0.0017) < 0.001 # the bias should be close to 0.007\n",
    "\n",
    "runtest(test_hbar1, 'test_hbar1')\n",
    "runtest(test_hbar2, 'test_hbar2')\n",
    "runtest(test_hbar3, 'test_hbar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e6d8b06aa693af3dc5c6151d9cad655",
     "grade": true,
     "grade_id": "cell-hbar1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1bd384a79608052db9dc59f73ca67b6",
     "grade": true,
     "grade_id": "cell-hbar2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a3f6f645ee79ca122b6123a5740bc6d",
     "grade": true,
     "grade_id": "cell-hbar3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_hbar3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "479e3b353b6824ce02548943a10f0a13",
     "grade": false,
     "grade_id": "cell-c26d1082ffdeed0c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Four: Bias Continued [Graded]\n",
    "\n",
    "Now we're equipped to calculate the bias $\\mathbb{E} \\left[ \\left( \\overline{h}(\\mathbf{x}) - \\overline{y}(\\mathbf{x}) \\right)^2 \\right]$. In this case, you need to compute the mean of the squared difference over your input points:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( \\overline{h}(\\mathbf{x}) - \\overline{y}(\\mathbf{x}) \\right)^2 \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n \\left( \\bar{h} \\left( \\mathbf{x}_i \\right) - \\overline{y} \\left( \\mathbf{x}_i \\right) \\right)^2\n",
    "$$\n",
    "\n",
    "You can call both `computehbar` and `computeybar` to calculate the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "37894e34549c4eac234e2dc8dacb9a61",
     "grade": false,
     "grade_id": "cell-computebias",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def computebias(xTe, depth, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    Computes the bias for data set xTe. \n",
    "    Each of the NMODELS regression trees used in the average tree should be\n",
    "    trained using data from toydata(OFFSET, Nsmall).\n",
    "\n",
    "    Input:\n",
    "        xTe       : data matrix of shape nx2\n",
    "        depth     : Depth of each regression tree to be trained\n",
    "        Nsmall    : Number of points in the dataset that each tree is trained on\n",
    "        NMODELS   : Number of regression trees to train\n",
    "        OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                    mu of normal distributions for points with labels class1 and class2.\n",
    "    \n",
    "    Output:\n",
    "        bias      : a scalar representing the bias component of the error of xTe\n",
    "    \"\"\"\n",
    "    bias = 0\n",
    "    # YOUR CODE HERE\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    # calculate bias\n",
    "    bias = (1/Nsmall) * np.sum(np.square(hbar - ybar))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f77f74049c71e090ef58292e588c6338",
     "grade": false,
     "grade_id": "cell-computebias-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_bias1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n)\n",
    "    bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isscalar(bias) # the dimension of hbar should be (n, )\n",
    "\n",
    "def test_bias2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 1\n",
    "    \n",
    "    # since the mean is far apart, the tree should be able to learn perfectly\n",
    "    bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isclose(bias, 0) # the bias should be close to zero\n",
    "\n",
    "def test_bias3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    bias = computebias(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    return np.abs(bias - 0.0017) < 0.001 # the bias should be close to 0.007\n",
    "\n",
    "runtest(test_bias1, 'test_bias1')\n",
    "runtest(test_bias2, 'test_bias2')\n",
    "runtest(test_bias3, 'test_bias3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3eaf3388fafe05175a818a95313b713c",
     "grade": true,
     "grade_id": "cell-test_bias1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4a905b6eebdd3467fd663fc8ea8d03eb",
     "grade": true,
     "grade_id": "cell-test_bias2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f182657b74933ab3f8881ae2e308878",
     "grade": true,
     "grade_id": "cell-test_bias3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_bias3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fdc99fbb55600c73447330781bd003ca",
     "grade": false,
     "grade_id": "cell-484ccd205f7b43fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Five: Variance [Graded]\n",
    "\n",
    "Finally, to compute the variance, you need to compute the term $\\mathbb{E} \\left[ \\left( h_D (\\mathbf{x}) - \\overline{h}(\\mathbf{x}) \\right)^2 \\right]$ in **`computevariance`**.\n",
    "\n",
    "The variance term is an expectation over $D$ and $\\mathbf{x}$ and we can take a two-step approach. First, we fix $\\mathbf{x}_i$ and calculate $\\overline{v} \\left( \\mathbf{x}_i \\right) = \\mathbb{E}_D \\left[ \\left( h_D \\left( \\mathbf{x}_i \\right) - \\overline{h} \\left( \\mathbf{x}_i \\right) \\right)^2 \\right]$ for each $\\mathbf{x}_i$. Once again, you cannot compute the expected value exactly, but you can approximate this term by averaging over $m$ (`NMODELS`) models. You can use your implementation of `computehbar` to compute $\\overline{h}$, and then estimate\n",
    "\n",
    "$$\n",
    "\\overline{v} \\left( \\mathbf{x}_i \\right) = \\mathbb{E}_D \\left[ \\left( h_D \\left( \\mathbf{x}_i \\right) - \\overline{h} \\left( \\mathbf{x}_i \\right) \\right)^2 \\right] \\approx \\frac{1}{m} \\sum_{j=1}^m \\left( h_{D_j} \\left( \\mathbf{x}_i \\right) - \\overline{h} \\left( \\mathbf{x}_i \\right) \\right)^2\n",
    "$$\n",
    "where once again $D_1, \\dots, D_m$ are i.i.d. training data sets of size $n$ (`Nsmall`). Note that the output of this function is a $n$-dimensional vector; in other words, you should not be taking an average over the $n$ input points.\n",
    "\n",
    "Finally, you will need to return the variance itself, which is\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( h_D (\\mathbf{x}) - \\overline{h}(\\mathbf{x}) \\right)^2 \\right] = \\mathbb{E}_\\mathbf{x} \\left[ \\overline{v}(\\mathbf{x}) \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n \\overline{v} \\left( \\mathbf{x}_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8fa8a023e9ad5b5e8f1d2ad1638b5d68",
     "grade": false,
     "grade_id": "cell-computevariance",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    Computes the variance of classifiers trained on data sets from toydata(OFFSET, Nsmall). \n",
    "    The prediction of the average classifier is assumed to be stored in \"hbar\".\n",
    "\n",
    "    Input:\n",
    "        xTe       : data matrix of shape nx2\n",
    "        depth     : Depth of each regression tree to be trained\n",
    "        hbar      : nx1 vector of the predictions of hbar on the inputs xTe\n",
    "        Nsmall    : Number of points in the dataset that each tree is trained on\n",
    "        NMODELS   : Number of regression trees to train\n",
    "        OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                    mu of normal distributions for points with labels class1 and class2.\n",
    "    \n",
    "    Output:\n",
    "        variance  : a scalar representing the bias component of the error of xTe\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    vbar = np.zeros(n)\n",
    "    variance = 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # calculate variance\n",
    "    hbar = np.zeros(n)\n",
    "    hD = np.zeros((n, NMODELS))\n",
    "    sse = 0\n",
    "    for i_model in range(NMODELS):\n",
    "        # get toy data\n",
    "        i_xTr, i_yTr = toydata(OFFSET, Nsmall)\n",
    "        # create decision tree\n",
    "        tree = RegressionTree(depth=depth)\n",
    "        # fit tree on sample i\n",
    "        tree.fit(i_xTr, i_yTr)\n",
    "        # predict on test\n",
    "        pred = tree.predict(xTe)\n",
    "        hD[:, i_model] = pred\n",
    "        hbar += pred\n",
    "        \n",
    "    hbar = (1/NMODELS) * hbar\n",
    "    # calculate sum squared errors\n",
    "    sse = np.zeros(n)\n",
    "    for i_model in range(NMODELS):\n",
    "        sse += np.square(hD[:, i_model] - hbar)\n",
    "    sse = (1/NMODELS) * sse\n",
    "    # calculate variance\n",
    "    variance = (1/n) * np.sum(sse)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31783d8a214175b44fa8852694ff58f2",
     "grade": false,
     "grade_id": "cell-computevariance-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_variance1():\n",
    "    OFFSET = 2\n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10 \n",
    "    n = 1000\n",
    "    xTe, yTe = toydata(OFFSET, n)\n",
    "    hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isscalar(var) # variance should be a scalar\n",
    "\n",
    "def test_variance2():\n",
    "    OFFSET = 50\n",
    "    # Create an easy dataset\n",
    "    # We set sigma=1 and since the mean is far apart,\n",
    "    # the noise is negligible\n",
    "    xTe = np.array([\n",
    "        [49.308783, 49.620651], \n",
    "        [1.705462, 1.885418], \n",
    "        [51.192402, 50.256330],\n",
    "        [0.205998, -0.089885],\n",
    "        [50.853083, 51.833237]])  \n",
    "    yTe = np.array([2, 1, 2, 1, 2])\n",
    "    \n",
    "    depth = 2\n",
    "    Nsmall = 10\n",
    "    NMODELS = 10\n",
    "    \n",
    "    # since the noise is negligible, the tree should be able to learn perfectly\n",
    "    hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.isclose(var, 0) # the bias should be close to zero\n",
    "\n",
    "def test_variance3():\n",
    "    OFFSET = 3;\n",
    "\n",
    "    xTe = np.array([\n",
    "        [0.45864, 0.71552],\n",
    "        [2.44662, 1.68167],\n",
    "        [1.00345, 0.15182],\n",
    "        [-0.10560, -0.48155],\n",
    "        [3.07264, 3.81535],\n",
    "        [3.13035, 2.72151],\n",
    "        [2.25265, 3.78697]])\n",
    "    yTe = np.array([1, 2, 1, 1, 2, 2, 2])\n",
    "    \n",
    "    depth = 3\n",
    "    Nsmall = 10\n",
    "    NMODELS = 100\n",
    "    \n",
    "    # set the random seed to ensure consistent behavior\n",
    "    np.random.seed(1)\n",
    "    # since the noise is negligible, the tree should be able to learn perfectly\n",
    "    hbar = computehbar_grader(xTe, depth, Nsmall, NMODELS, OFFSET) \n",
    "    var = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    return np.abs(var - 0.0404) < 0.0015 # the variance should be close to 0.0404\n",
    "\n",
    "runtest(test_variance1, 'test_variance1')\n",
    "runtest(test_variance2, 'test_variance2')\n",
    "runtest(test_variance3, 'test_variance3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff4b521ac026a0b4f71348f116ff372e",
     "grade": true,
     "grade_id": "cell-var1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acef3144d0efd73c9ecbf10b2f46d325",
     "grade": true,
     "grade_id": "cell-var2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16fc290b85507965a6eef1c476c080e4",
     "grade": true,
     "grade_id": "cell-var3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs test_variance3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8730ccf2ee31450de1a66f742df219cf",
     "grade": false,
     "grade_id": "cell-3284bddfa0d87f94",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Review the Error for Various Depths</h3>\n",
    "\n",
    "<p>If you did everything in the three previous graded sections correctly and execute the following cell, you should see how the error decomposes (roughly) into bias, variance and noise for various depths.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# biasvariancedemo\n",
    "\n",
    "OFFSET = 1.75\n",
    "# how big is the training set size N\n",
    "Nsmall = 75\n",
    "# how big is a really big data set (approx. infinity)\n",
    "Nbig = 7500\n",
    "# how many models do you want to average over\n",
    "NMODELS = 100\n",
    "# What regularization constants to evaluate\n",
    "depths = [0, 1, 2, 3, 4, 5, 6, np.inf]\n",
    "\n",
    "# we store\n",
    "Ndepths = len(depths)\n",
    "lbias = np.zeros(Ndepths)\n",
    "lvariance = np.zeros(Ndepths)\n",
    "ltotal = np.zeros(Ndepths)\n",
    "lnoise = np.zeros(Ndepths)\n",
    "lsum = np.zeros(Ndepths)\n",
    "\n",
    "# Different regularization constant classifiers\n",
    "for i in range(Ndepths):\n",
    "    depth = depths[i]\n",
    "    # use this data set as an approximation of the true test set\n",
    "    xTe,yTe = toydata(OFFSET, Nbig)\n",
    "    \n",
    "    # Estimate AVERAGE ERROR (TOTAL)\n",
    "    total = 0\n",
    "    for j in range(NMODELS):\n",
    "        # Set the seed for consistent behavior\n",
    "        xTr2,yTr2 = toydata(OFFSET, Nsmall)\n",
    "        model = RegressionTree(depth=depth)\n",
    "        model.fit(xTr2, yTr2)\n",
    "        total += np.mean((model.predict(xTe) - yTe) ** 2)\n",
    "    total /= NMODELS\n",
    "    \n",
    "    # Estimate Noise\n",
    "    noise = computenoise(xTe, yTe, OFFSET)\n",
    "    \n",
    "    # Estimate Bias\n",
    "    bias = computebias(xTe,depth,Nsmall, NMODELS, OFFSET)\n",
    "    \n",
    "    # Estimating VARIANCE\n",
    "    hbar = computehbar(xTe, depth, Nsmall, NMODELS, OFFSET)\n",
    "    variance = computevariance(xTe, depth, hbar, Nsmall, NMODELS, OFFSET)\n",
    "    \n",
    "    # print and store results\n",
    "    lbias[i] = bias\n",
    "    lvariance[i] = variance\n",
    "    ltotal[i] = total\n",
    "    lnoise[i] = noise\n",
    "    lsum[i] = lbias[i]+lvariance[i]+lnoise[i]\n",
    "    \n",
    "    if np.isinf(depths[i]):\n",
    "        print('Depth infinite: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (lbias[i],lvariance[i],lnoise[i],lsum[i],ltotal[i]))\n",
    "    else:\n",
    "        print('Depth: %d: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (depths[i],lbias[i],lvariance[i],lnoise[i],lsum[i],ltotal[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(lbias[:Ndepths], '*', c='r',linestyle='-',linewidth=2)\n",
    "plt.plot(lvariance[:Ndepths], '*', c='g', linestyle='-',linewidth=2)\n",
    "plt.plot(lnoise[:Ndepths], '*', c='b',linestyle='-',linewidth=2)\n",
    "plt.plot(ltotal[:Ndepths], '*', c='orange', linestyle='-',linewidth=2)\n",
    "plt.plot(lsum[:Ndepths], '*', c='k', linestyle='dotted',linewidth=2)\n",
    "\n",
    "plt.legend([\"Bias\",\"Variance\",\"Noise\",\"Test error\",\"Bias+Var+Noise\"]);\n",
    "plt.xlabel(\"Depth\",fontsize=18);\n",
    "plt.ylabel(\"Squared Error\",fontsize=18);\n",
    "plt.xticks([i for i in range(Ndepths)], depths);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}