{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02ea6ea566df7354a9a88bcce4c6e714",
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement the CART (Classification and Regression Tree) algorithm. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">Ionosphere Data Set from the UCI Machine Learning Repository</a>, which consists of radar data from a system designed to target free electrons in the ionosphere, and a artificial \"spiral\" dataset. The first dataset will be used to determine if a radar return was \"good\" (i.e. a signal was returned) or \"bad\" (i.e. the signal passed straight through the ionosphere).\n",
    "\n",
    "**You will be using a regression tree with squared loss impurity to do classification. This is possible here because all classification problems can be framed as regression problems. You could also have used a classification tree with Gini impurity equivalently.**\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf891c1be4dc1ae06191273f05e0d68e",
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Implementing Regression Trees</h2>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. In addition, you will load two binary classification dataset - the spiral dataset and the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ad00e0439033b41698216da17731123",
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a36ccf5bfced96b28d8966f36446473",
     "grade": false,
     "grade_id": "cell-c219552fd154672d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9495e5cadae1976dadd4b1ad0e63367c",
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N) # generate a vector of \"radius\" values\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T # generate a curve that draws circles with increasing radius\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    # Now sample alternating values to generate the test and train sets.\n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr, yTr, xTe, yTe\n",
    "\n",
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "print(f'Number of training points in spiral dataset: {xTrSpiral.shape[0]}')\n",
    "print(f'Number of testing points in spiral dataset: {xTeSpiral.shape[0]}')\n",
    "print(f'Number of features in spiral dataset: {xTrSpiral.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66d9205e7c90b804b83fed62414fed1e",
     "grade": false,
     "grade_id": "cell-65f296cbe2f8b1ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each data point $[\\mathbf{x}]_i$ in the spiral data has 2 dimensions and the label $y_i$ is either $-1$ or $+1$. We can plot `xTrSpiral` to see the points, colored by the label they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(xTrSpiral[:, 0], xTrSpiral[:, 1], s=30, c=yTrSpiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3183e35d16b85a5f474e39a15cd14f5e",
     "grade": false,
     "grade_id": "cell-d73f1d288cf74deb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following code loads the ION dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8223c9331576477c95d446780580ff1",
     "grade": false,
     "grade_id": "cell-134b99a8d0c69131",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()\n",
    "\n",
    "print(f'Number of training points in ION dataset: {xTrIon.shape[0]}')\n",
    "print(f'Number of testing points in ION dataset: {xTeIon.shape[0]}')\n",
    "print(f'Number of features in ION dataset: {xTrIon.shape[1]}')\n",
    "print('Training set: (n x d matrix)')\n",
    "TrIon_for_display = np.concatenate([yTrIon[:, None], xTrIon], axis=1)\n",
    "TrIon_for_display = TrIon_for_display[TrIon_for_display[:, 0].argsort()]\n",
    "\n",
    "display(pd.DataFrame(data=TrIon_for_display,\n",
    "                     columns=['$y$'] + [f'$[\\mathbf{{x}}]_{ {i+1} }$' for i in range(xTrIon.shape[1])]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1471471f16e7153e9f7916d8893fab91",
     "grade": false,
     "grade_id": "cell-c632b2561da0c25c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part One: Implement `sqimpurity` [Graded]\n",
    "\n",
    "First, implement the function **`sqimpurity`**, which takes as input a vector $y$ of $n$ labels and outputs the corresponding squared loss impurity:\n",
    "$$\n",
    "\\sum_{i = 1}^{n} \\left( y_i - \\overline{y} \\right)^2 \\textrm{, where } \\overline{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
    "$$\n",
    "\n",
    "Again, the squared loss impurity works fine even though our final objective is classification. This is because the labels are binary and classification problems can be framed as regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "def430a6952c11c5e4cf8b3d9516fe81",
     "grade": false,
     "grade_id": "cell-ec2301f1325f79b5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sqimpurity(yTr):\n",
    "    \"\"\"\n",
    "    Computes the squared loss impurity (variance) of the labels.\n",
    "    \n",
    "    Input:\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        squared loss impurity: weighted variance/squared loss impurity of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    N, = yTr.shape\n",
    "    assert N > 0 # must have at least one sample\n",
    "    # YOUR CODE HERE\n",
    "    # as suggested in lecture, decompose squared loss in two parts\n",
    "    q = np.sum(np.square(yTr))\n",
    "    s = np.sum(yTr)\n",
    "    impurity = q - np.square(s) / N\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28e61f2dc5aeb81315a4e40b9306e92d",
     "grade": false,
     "grade_id": "cell-ba20a92528e16fbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sqimpurity_test1():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isscalar(impurity)  # impurity should be scalar\n",
    "\n",
    "def sqimpurity_test2():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return impurity >= 0 # impurity should be nonnegative\n",
    "\n",
    "def sqimpurity_test3():\n",
    "    yTr = np.ones(100) # generate an all one vector as labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isclose(impurity, 0) # impurity should be zero since the labels are homogeneous\n",
    "\n",
    "def sqimpurity_test4():\n",
    "    yTr = np.arange(-5, 6) # generate a vector with mean zero\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    sum_of_squares = np.sum(yTr ** 2) \n",
    "    return np.isclose(impurity, sum_of_squares) # with mean zero, then the impurity should be the sum of squares\n",
    "\n",
    "def sqimpurity_test5():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr)\n",
    "    impurity_grader = sqimpurity_grader(yTr)\n",
    "    return np.isclose(impurity, impurity_grader)\n",
    "\n",
    "runtest(sqimpurity_test1, 'sqimpurity_test1')\n",
    "runtest(sqimpurity_test2, 'sqimpurity_test2')\n",
    "runtest(sqimpurity_test3, 'sqimpurity_test3')\n",
    "runtest(sqimpurity_test4, 'sqimpurity_test4')\n",
    "runtest(sqimpurity_test5, 'sqimpurity_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e4b6cdeffb9e6abb0e20f1426d5e335",
     "grade": true,
     "grade_id": "cell-84a2790fead7b76f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "295c7921f17373bcee865218851434f0",
     "grade": true,
     "grade_id": "cell-835351931df18818",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15be06d818d669ee3d72cd31f9849813",
     "grade": true,
     "grade_id": "cell-3c3b7a31a818b505",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "465b390daf35d5c677b377e4a98633ee",
     "grade": true,
     "grade_id": "cell-1c7dc9e1879e6a32",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0906343847ad8e1e8800b1c85bd474a7",
     "grade": true,
     "grade_id": "cell-330bca5d42fef37a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "110d11c6a9b86f38e65eb9d517778dac",
     "grade": false,
     "grade_id": "cell-c8238fba4c3de7ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's plot the shape of the impurity function. We vary the mixture of labels in a set of $n$ labels and calculate the impurity of the labels. When the labels are mostly the same, the impurity should be low. When the labels are evenly split between $+1$ and $-1$, the impurity should be the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "size = 10\n",
    "y = np.ones(size)\n",
    "fraction_pos, impurities = [], []\n",
    "for i in range(size):\n",
    "    fraction_pos.append(sum([y == 1]) / size)\n",
    "    impurities.append(sqimpurity(y))\n",
    "    y[i] = -1\n",
    "fraction_pos.append(sum([y == 1]) / size)\n",
    "impurities.append(sqimpurity(y))\n",
    "\n",
    "display(pd.DataFrame(data={'fraction_pos': fraction_pos, 'impurity': impurities}))\n",
    "plt.plot(fraction_pos, impurities)\n",
    "plt.grid()\n",
    "plt.xlabel('Fraction of positive labels (1 - fraction of negative labels)')\n",
    "plt.ylabel('Squared loss impurity of labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "27c84f41b6f8c383bd75fccc138ad42c",
     "grade": false,
     "grade_id": "cell-4730d6e94ace8e06",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Two: Implement `sqsplit` [Graded]\n",
    "\n",
    "Now implement **`sqsplit`**, which takes as input a data set of size $n \\times d$ with labels and computes the best feature and the threshold/cut of the optimal split based on the squared loss impurity. The function outputs a feature dimension `0 <= feature < d`, a cut threshold `cut`, and the impurity loss `bestloss` of this best split.\n",
    "\n",
    "Recall in the CART algorithm that, to find the split with the minimum impurity, you iterate over all features and cut values along each feature. We enforce that the cut value be the average of the two consecutive data points' feature values.\n",
    "\n",
    "You should calculate the impurity of a node of data $S$ with two branches $S_L$ and $S_R$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(S) &= \\frac{\\left| S_L \\right|}{|S|} I \\left( S_L \\right) + \\frac{\\left| S_R \\right|}{|S|} I \\left( S_R \\right)\\\\\n",
    "&= \\frac{1}{|S|}\\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\frac{1}{|S|} \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\\\\\n",
    "&\\propto \\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Implementation Notes:**\n",
    "- For calculating the impurity of a node, you should just return the sum of left and right impurities instead of the average.\n",
    "- Returned `feature` must be 0-indexed as is consistent with programming in Python.\n",
    "- If along a feature $f$, two data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ have the same value, avoid splitting between them; move to the next pair of data points.\n",
    "\n",
    "For example, with the following `xTr` of size $4 \\times 3$ and `yTr` for 4 points:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2\\\\\n",
    "2 & 0 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "among possible features `[0, 1, 2]`, the best split would be at `feature = 1` and `cut = (0 + 1) / 2 = 0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you're stuck, we recommend that you start with the naïve algorithm for finding the best split, which involves a double loop over all features `0 <= f < d` and all cut values `xTr[0, f] < (xTr[i, f] + xTr[i+1, f]) / 2 < xTr[n-1, f]` (with `xTr` sorted along feature `f`). This algorithm thus calculates impurities for `d(n-1)` splits. Here's the pseudocode:\n",
    "\n",
    "<center><img src=\"cart-id3_best_split_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9033ab9fb3dbbcb866dcc1bd45db8f4",
     "grade": false,
     "grade_id": "cell-sqsplit",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Finds the best feature, cut value, and impurity for a split of (xTr, yTr) based on squared loss impurity.\n",
    "    \n",
    "    Input:\n",
    "        xTr: n x d matrix of data points\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature (keep in mind this is 0-indexed)\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: squared loss impurity of the best cut\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    assert d > 0 # must have at least one dimension\n",
    "    assert n > 1 # must have at least two samples\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # initial solution\n",
    "    loss_star = np.inf\n",
    "    feature_star = np.inf\n",
    "    cut_star = np.inf\n",
    "    \n",
    "    # start iteration over features\n",
    "    for i_d in range(d):\n",
    "        # get values for feature i_d \n",
    "        f_values = xTr[:, i_d]\n",
    "        # sort values for feature _id\n",
    "        f_values_sorted = f_values[np.argsort(f_values)]\n",
    "        # start splitting procedure\n",
    "        for i_n in range(n-1):\n",
    "            if not np.isclose(f_values_sorted[i_n], f_values_sorted[i_n+1]):  # avoid splitting if values are the same\n",
    "                # calculate cut value\n",
    "                cut_i_n = (f_values_sorted[i_n] + f_values_sorted[i_n+1])/2\n",
    "                # get index and values for L and R split\n",
    "                y_L_idx = f_values <= cut_i_n\n",
    "                y_L = yTr[y_L_idx]\n",
    "                y_R = yTr[~y_L_idx]\n",
    "                loss_i_n = sqimpurity(y_L) + sqimpurity(y_R)\n",
    "                if loss_i_n < loss_star:\n",
    "                    # update\n",
    "                    loss_star = loss_i_n\n",
    "                    cut_star = cut_i_n\n",
    "                    feature_star = i_d\n",
    "\n",
    "    return feature_star, cut_star, loss_star  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbc3579da8f3891cc0b907caf6f397f",
     "grade": false,
     "grade_id": "cell-sqsplit_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The tests below check that your sqsplit function returns the correct values for several different input datasets\n",
    "\n",
    "t0 = time.time()\n",
    "fid, cut, loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f \\n\" % (fid,cut))\n",
    "\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4, yor4)[2] / len(yor4), .25)\n",
    "    b = np.isclose(sqsplit(xor3, yor3)[2] / len(yor3), .25)\n",
    "    c = np.isclose(sqsplit(xor2, yor2)[2] / len(yor2), .25)\n",
    "    return a and b and c\n",
    "\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    _, cut, _ = sqsplit(x, y)\n",
    "    return cut <= 500 or cut >= 499\n",
    "\n",
    "def sqsplit_test3():\n",
    "    fid, cut, loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut >= 0 or cut <= 1) and np.isclose(loss / len(yor5), 2/3)\n",
    "\n",
    "runtest(sqsplit_test1,'sqsplit_test1')\n",
    "runtest(sqsplit_test2,'sqsplit_test2')\n",
    "runtest(sqsplit_test3,'sqsplit_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cbffe6477a05dd8d9db574f0d94edcfa",
     "grade": true,
     "grade_id": "cell-sqsplit_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b0da843388d7a9ddf59c80d408d3859",
     "grade": true,
     "grade_id": "cell-sqsplit_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86d84abb3781b6b263676a20c5ed6418",
     "grade": true,
     "grade_id": "cell-sqsplit_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d358dfc0f25657b18024284428aec959",
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Three: Implement `cart` [Graded]\n",
    "\n",
    "In this section, you will implement the function **`cart`**, which returns a regression tree based on the minimum squared loss splitting rule. You should use the function `sqsplit` to make your splits.\n",
    "\n",
    "**Implementation Notes:**\n",
    "We've provided a tree structure in the form of `TreeNode` for you that can be used for both leaves and nodes. To represent the leaves, you would set all fields except `prediction` to `None`.\n",
    "\n",
    "Non-leaf nodes will have non-`None` fields for all except `prediction`:\n",
    "1. `left`: node describing left subtree\n",
    "2. `right`: node describing right subtree\n",
    "3. `feature`: index of feature to cut (0-indexed as returned by `sqsplit`)\n",
    "4. `cut`: cutoff value $t$ ($\\leq t$: left and $> t$: right)\n",
    "5. `prediction`: `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0903afbf0f71752d3ae3caa2a24e8360",
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"\n",
    "    Tree class.\n",
    "    \n",
    "    (You don't _need_ to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, feature, cut, prediction):\n",
    "        # Check that all or no arguments are None\n",
    "        node_or_leaf_args = [left, right, feature, cut]\n",
    "        assert all([arg == None for arg in node_or_leaf_args]) or all([arg != None for arg in node_or_leaf_args])\n",
    "        \n",
    "        # Check that all None <==> leaf <==> prediction not None\n",
    "        # Check that all non-None <==> non-leaf <==> prediction is None\n",
    "        if all([arg == None for arg in node_or_leaf_args]):\n",
    "            assert prediction is not None\n",
    "        if all([arg != None for arg in node_or_leaf_args]):\n",
    "            assert prediction is None\n",
    "        \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.feature = feature \n",
    "        self.cut = cut\n",
    "        self.prediction = prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eac0b2166f8fddde8c51b4ab8470e47",
     "grade": false,
     "grade_id": "cell-5b554dfec9394ba9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following cell contains some examples of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following is a tree that predicts everything as zero ==> prediction 0\n",
    "# In this case, it has no left or right children (it is a leaf node) ==> left = None, right = None, feature = None, cut = None\n",
    "root = TreeNode(None, None, None, None, 0)\n",
    "\n",
    "\n",
    "# The following that a tree with depth 2 or a tree with one split \n",
    "\n",
    "# The tree will return a prediction of 1 if an example falls under the left subtree\n",
    "# Otherwise it will return a prediction of 2\n",
    "# To start, first create two leaf node\n",
    "left_leaf = TreeNode(None, None, None, None, 1)\n",
    "right_leaf = TreeNode(None, None, None, None, 2)\n",
    "\n",
    "# Now create the parent or the root\n",
    "# Suppose we split at feature 0 and cut of 1 and the prediction is None\n",
    "root2 = TreeNode(left_leaf, right_leaf, 0, 1, None)\n",
    "\n",
    "# Now root2 is the tree we desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c2c5f1f300c14fa009977cd29781d7c",
     "grade": false,
     "grade_id": "cell-d6c63e37fd6c395f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now implement the function `cart` using **recursion** (you call `cart` on the left and right subtrees inside the `cart` function). Recall the pseudocode for the CART algorithm.\n",
    "\n",
    "**NOTE:** In this implementation, you will be using **`np.mean`** for `prediction` argument. To check that floating point values in `xTr` are the same or not, you can use `np.isclose(xTr, xTr[0])`, which returns a list of `True` and `False` based on how different the rows of `xTr` are from the vector `xTr[0]`.\n",
    "\n",
    "<center><img src=\"cart-id3_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de3750e532687607733d83dcc9a0a2dc",
     "grade": false,
     "grade_id": "cell-cart",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Builds a CART tree.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "\n",
    "    Output:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    node = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # calculate current prediction\n",
    "    pred = np.mean(yTr)\n",
    "    \n",
    "    if np.all(np.isclose(xTr, xTr[0])) or np.all(yTr == yTr[0]):\n",
    "        # return leaf\n",
    "        tree = TreeNode(None, None, None, None, pred)\n",
    "    else:\n",
    "        # get feature_star and cut_star\n",
    "        feature_star, cut_star, _ = sqsplit(xTr, yTr)\n",
    "        f_values = xTr[:, feature_star]\n",
    "        # get indices\n",
    "        idx_left = f_values <= cut_star\n",
    "        # calculate left and right leaf (recursive application here since we call cart within cart)\n",
    "        left_leaf = cart(xTr[idx_left,:], yTr[idx_left])\n",
    "        right_leaf = cart(xTr[~idx_left,:], yTr[~idx_left])\n",
    "        # return root\n",
    "        tree = TreeNode(left_leaf, right_leaf, feature_star, cut_star, None)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc022deb75c9f2be5071b44c001e6d82",
     "grade": false,
     "grade_id": "cell-cart_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The tests below check that your implementation of cart  returns the correct predicted values for a sample dataset\n",
    "\n",
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "#test case 4\n",
    "def cart_test4():\n",
    "    X = np.ones((5, 2)) # Create a dataset with identical examples\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # On this dataset, your cart algorithm should return a single leaf\n",
    "    # node with prediction equal to 1\n",
    "    t = cart(X, y)\n",
    "    \n",
    "    # t has no children\n",
    "    children_check = (t.left is None) and (t.right is None) \n",
    "    \n",
    "    # Make sure t does not cut any feature and at any value\n",
    "    feature_check = (t.feature is None) and (t.cut is None)\n",
    "    \n",
    "    # Check t's prediction\n",
    "    prediction_check = np.isclose(t.prediction, 1)\n",
    "    return children_check and feature_check and prediction_check\n",
    "\n",
    "#test case 5\n",
    "def cart_test5():\n",
    "    X = np.arange(4).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1])\n",
    "\n",
    "    t = cart(X, y) # your cart algorithm should generate one split\n",
    "    \n",
    "    # check whether you set t.feature and t.cut to something\n",
    "    return t.feature is not None and t.cut is not None\n",
    "\n",
    "\n",
    "\n",
    "runtest(cart_test1,'cart_test1')\n",
    "runtest(cart_test2,'cart_test2')\n",
    "runtest(cart_test3,'cart_test3')\n",
    "runtest(cart_test4,'cart_test4')\n",
    "runtest(cart_test5,'cart_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b47e3031bb2beb61cae1c347d53b283",
     "grade": true,
     "grade_id": "cell-cart_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e322a7a1bab0a83d65ad27517c275cab",
     "grade": true,
     "grade_id": "cell-cart_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d6bf0051fdf9a7296e893eb66c16b96",
     "grade": true,
     "grade_id": "cell-cart_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07b92c1bbd6474aa04712ddb4dbcc0ca",
     "grade": true,
     "grade_id": "cell-cart_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3a55c57681ae193e2771512f553e528",
     "grade": true,
     "grade_id": "cell-cart_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faf306bddae40ade7985dcf6439b861c",
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Four: Implement `evaltree` [Graded]\n",
    "\n",
    "Implement the function **`evaltree`**, which evaluates a decision tree on a given test data set. You essentially need to traverse the tree until you end up in a leaf, where you return the `prediction` value of the leaf. Like the `cart` function, you can call `evaltree` on the left subtree and right subtree on testing points that fall in the corresponding subtrees.\n",
    "\n",
    "Here's some inspiration:\n",
    "1. If the `tree` is a leaf, i.e. the left and right subtrees are `None`, return `tree.prediction` for all `m` testing points.\n",
    "2. If the `tree` is non-leaf, using `tree.feature` and `tree.cut` find testing points with the feature value less than/equal to the threshold and greater than. Now, you can call `evaltree` on `tree.left` and the left set of testing points to obtain the left set's predictions. Then obtain the predictions for the right set, and return all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f3f523c3529265fb0ce24f6fea9361",
     "grade": false,
     "grade_id": "cell-evaltree",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(tree, xTe):\n",
    "    \"\"\"\n",
    "    Evaluates testing points in xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        tree: TreeNode decision tree\n",
    "        xTe:  m x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: m-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m, d = xTe.shape\n",
    "    preds = np.zeros(m)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if tree.left is None and tree.right is None:\n",
    "        # tree is a leaf, return predicition\n",
    "        preds[:] = tree.prediction\n",
    "    else:\n",
    "        # tree is not a leaf -> traverse further\n",
    "        f_values = xTe[:, tree.feature]\n",
    "        # get indices for split\n",
    "        idx_left = f_values <= tree.cut\n",
    "        # do split\n",
    "        xTe_left = xTe[idx_left, :]\n",
    "        xTe_right = xTe[~idx_left, :]\n",
    "        # eval tree left and right\n",
    "        pred_left = evaltree(tree.left, xTe_left)\n",
    "        pred_right = evaltree(tree.right, xTe_right)\n",
    "        # put predictions back together\n",
    "        preds[idx_left] = pred_left\n",
    "        preds[~idx_left] = pred_right\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59551843c4d5ab8b8c0924087bb9202c",
     "grade": false,
     "grade_id": "cell-evaltree_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of evaltree returns the correct predictions for two sample trees\n",
    "\n",
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)\n",
    "\n",
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, 0, 10, None)\n",
    "    y = TreeNode(x, c, 0, 20, None)\n",
    "    z = TreeNode(d, e, 0, 40, None)\n",
    "    t = TreeNode(y, z, 0, 30, None)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "runtest(evaltree_test1,'evaltree_test1')\n",
    "runtest(evaltree_test2,'evaltree_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d772deb41878ca9f83b8c3661ac75b67",
     "grade": true,
     "grade_id": "cell-evaltree_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68d89de2748bcdc384eb209588ffb84d",
     "grade": true,
     "grade_id": "cell-evaltree_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfac59f41d855c0f8b4e1e7bf394c7d",
     "grade": false,
     "grade_id": "cell-030c38de62d6adc6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Training and Testing a Classification Tree on the ION Dataset</h3>\n",
    "\n",
    "<p> The following code create a classification tree on the ION dataset and then apply the learned tree to an unknown dataset. If you implement everything correctly, you should get a training RSME that is close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efc7b56e0189a0d123d3ac40b1011afa",
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> Visualize Your Tree</h3>\n",
    "\n",
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694a973d8d7c2709f769a08c64cf8f0f",
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=None,b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w is not None:\n",
    "        w = np.array(w).flatten()\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X), xTrSpiral, yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1fe9d13867759f9403c562fe900853a",
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Click to add positive points and use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn also provides an implementation of [Regression Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) (and [Decision Tree Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). The usage is pretty straight-forward: define the regression tree with the impurity function (and other settings), fit to the training set, and evaluate on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "t0 = time.time()\n",
    "tree = DecisionTreeRegressor(\n",
    "    criterion='mse', # Impurity function = Mean Squared Error (squared loss)\n",
    "    splitter='best', # Take the best split\n",
    "    max_depth=None, # Expand the tree to the maximum depth possible\n",
    ")\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((tree.predict(xTrSpiral) - yTrSpiral)**2)\n",
    "te_err   = np.mean((tree.predict(xTeSpiral) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scikit-learn also provides a tree plotting function, which is again quite simple to use. This is extremely useful while debugging a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "_ = plot_tree(tree, ax=ax, precision=2, feature_names=[f'$[\\mathbf{{x}}]_{i+1}$' for i in range(2)], filled=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}