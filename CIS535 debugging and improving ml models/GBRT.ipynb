{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "340d362f4f41880458c33073035abba4",
     "grade": false,
     "grade_id": "cell-138bf686eef190d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "In this project, you will implement Gradient Boosted Regression Trees (GBRTs).\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f8fd9147ecd4b7abb8bda86745aad6f2",
     "grade": false,
     "grade_id": "cell-cfd4728219aa2e6f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "from helper import *\n",
    "%matplotlib inline\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1a3d019a7d57fc058d4c25bba5a1573",
     "grade": false,
     "grade_id": "cell-6613495f7b984ddf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this project, we will work with an artificial 2D spiral dataset of size 150 (for visualization), and a high dimensional dataset [ION](https://archive.ics.uci.edu/ml/datasets/Ionosphere) (for a binary test classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9931e62fe1d8f589c5db62eea6e0478b",
     "grade": false,
     "grade_id": "cell-ea9d14b9dc5c69e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "xTrIon, yTrIon, xTeIon, yTeIon = iondata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33fda24ec71d47ccb39666f0f0ec6d26",
     "grade": false,
     "grade_id": "cell-b74b5db289644016",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You will use the regression tree from a previous project. As a reminder, the following code shows you how to instantiate a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f71cacf5d5a056b48006f77982a266c",
     "grade": false,
     "grade_id": "cell-425caeb78952a935",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a regression tree with depth 4\n",
    "tree = RegressionTree(depth=4)\n",
    "\n",
    "# To fit/train the regression tree\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to predict a score for the example\n",
    "score = tree.predict(xTrSpiral)\n",
    "\n",
    "# To use the trained regression tree to make a +1/-1 prediction\n",
    "pred = np.sign(tree.predict(xTrSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43f8c148f7be217c9955f07120447997",
     "grade": false,
     "grade_id": "cell-ea4ccaa772dfade7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now run the following cell that evaluates your `depth=4` decision tree on the training set and test set. You might see that the difference between training error and test error is small (no overfitting) but both the errors are rather high. This is a sign of underfitting or high bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the depth 4 decision tree\n",
    "# tr_err   = np.mean((np.sign(tree.predict(xTrSpiral)) - yTrSpiral)**2)\n",
    "# te_err   = np.mean((np.sign(tree.predict(xTeSpiral)) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(tree.predict(xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(tree.predict(xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d63bb2bd4111011f9cc976962fcb81e0",
     "grade": false,
     "grade_id": "cell-a219c78df15f9ea0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To convince you further that the `depth=4` tree is underfitting, we create the following function `visclassifier()`, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the spiral data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c4cae9043250cdf463e3719bd2ab3e4",
     "grade": false,
     "grade_id": "cell-ea356e95d6f8a95d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,newfig=True):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    if newfig:\n",
    "        plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "                   )\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "visclassifier(lambda X: tree.predict(X),xTrSpiral,yTrSpiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c19d130f07a7e92941280a737e24818d",
     "grade": false,
     "grade_id": "cell-ddb0a80e1c5ac5ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see from the graph above, the `depth=4` tree is too simple and is not capable to capture the inherit information of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eee61aee4f2a547d9d16469003c8bae6",
     "grade": false,
     "grade_id": "cell-a8fd45efb4de5bac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient Boosting Regression Tree in Action\n",
    "\n",
    "### Part One: Implement `evalboostforest` [Graded]\n",
    "\n",
    "CART trees are known to be high variance classifiers if trained to full depth. However, if you build trees of small depth (e.g. only `depth=3` or `depth=4`), the trees do not overfit as we see above. Instead, they suffer from **high bias** and thus **underfit**. We can reduce the bias of a classifier with gradient boosting.\n",
    "\n",
    "Before implementing Gradient Boosted Regression Trees, you will implement **`evalboostforest`** with an additional argument `alphas` that lets you weigh the trees' predictions with any weights (in bagging all weights `alpha[i]` were equal).\n",
    "\n",
    "Similar to bagging, the `evalboostforest` is provided a list of $m$ trees $\\left[ h_1, \\dots, h_m \\right]$ and $n$ $d$-dimensional data points. However, this time, the function will weigh each tree $h_j$ according to $\\alpha_j$.\n",
    "\n",
    "More precisely, for each test sample $\\mathbf{x}_i$, `evalboostforest` should output the following prediction:\n",
    "\n",
    "$$\n",
    "H \\left( \\mathbf{x}_i \\right) = \\sum_{j=1}^m \\alpha_j h_j \\left( \\mathbf{x}_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1d282e4080802917fd2540256c062f53",
     "grade": false,
     "grade_id": "cell-evalboostforest",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evalboostforest(trees, X, alphas=None):\n",
    "    \"\"\"\n",
    "    Evaluates data points X using trees using weights alphas (optional).\n",
    "    \n",
    "    Input:\n",
    "        trees:  list of length m of RegressionTree decision trees\n",
    "        X:      n x d matrix of data points\n",
    "        alphas: m-dimensional weight vector for the ensemble's prediction\n",
    "        \n",
    "    Output:\n",
    "        pred:   n-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m = len(trees)\n",
    "    n,d = X.shape\n",
    "    \n",
    "    if alphas is None:\n",
    "        alphas = np.ones(m) / len(trees)\n",
    "            \n",
    "    pred = np.zeros(n)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # loop over trees, calculate prediction and weighted average prediction\n",
    "    for i_tree, v_tree in enumerate(trees):\n",
    "        score = v_tree.predict(X)\n",
    "        pred += alphas[i_tree] * score\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aeece350f0ff0389c419b4904adc9b7",
     "grade": false,
     "grade_id": "cell-a9dcb09b57da887e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evalboostforest_test0():\n",
    "    m = 200\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees = forest(x, y, m) # create a list of trees \n",
    "    preds = evalboostforest(trees, x)\n",
    "    return preds.shape == y.shape\n",
    "\n",
    "def evalboostforest_test1():\n",
    "    m = 200\n",
    "    x = np.random.rand(10,3)\n",
    "    y = np.ones(10)\n",
    "    x2 = np.random.rand(10,3)\n",
    "\n",
    "    max_depth = 0\n",
    "    \n",
    "    # Create a forest with trees depth 0\n",
    "    # Since the data are all ones, each tree will return 1 as prediction\n",
    "    trees = forest(x, y, m, max_depth) # create a list of trees      \n",
    "    pred = evalboostforest(trees, x2)[0]\n",
    "    return np.isclose(pred,1)  # the prediction should be equal to the sum of weights\n",
    "\n",
    "def evalboostforest_test2(): \n",
    "    # results should match evalforest if alphas are 1/m and labels +1, -1\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.sign(np.arange(100))\n",
    "    trees = forest(x, y, m) # create a list of m trees \n",
    "\n",
    "    alphas=np.ones(m)/m\n",
    "    preds1 = evalforest(trees, x) #evaluate the forest using our own implementation\n",
    "    preds2 = evalboostforest(trees, x, alphas)\n",
    "    return np.all(np.isclose(preds1,preds2))\n",
    "\n",
    "def evalboostforest_test3(): \n",
    "    # if only alpha[i]=1 and all others are 0, the result should match exactly \n",
    "    # the predictions of the ith tree\n",
    "    m = 20\n",
    "    x = np.random.rand(100,5)\n",
    "    y = np.arange(100)\n",
    "    x2 = np.random.rand(20,5)\n",
    "\n",
    "    trees = forest(x, y, m)  # create a list of m trees\n",
    "    allmatch=True\n",
    "    for i in range(m): # go through each tree i\n",
    "        alphas=np.zeros(m)\n",
    "        alphas[i]=1.0; # set only alpha[i]=1 all other alpha=0\n",
    "        preds1 = trees[i].predict(x2) # get prediction of ith tree\n",
    "        preds2 = evalboostforest(trees, x2, alphas) # get prediction of weighted ensemble\n",
    "        allmatch=allmatch and all(np.isclose(preds1,preds2))\n",
    "    return allmatch\n",
    "\n",
    "\n",
    "# and some new tests to check if the weights (and the np.sign function) were incorporated correctly \n",
    "runtest(evalboostforest_test0, 'evalboostforest_test0')\n",
    "runtest(evalboostforest_test1, 'evalboostforest_test1')\n",
    "runtest(evalboostforest_test2, 'evalboostforest_test2')\n",
    "runtest(evalboostforest_test3, 'evalboostforest_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c4019fedf3cbe5e0bc67a019f7727b5f",
     "grade": true,
     "grade_id": "cell-f3e96ee243300b0b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalboostforest_test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98748fc2f5dd8cfe91d1b358efa9c974",
     "grade": true,
     "grade_id": "cell-d350f889a32e3d98",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalboostforest_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f6b5f29bcdcec1acd8783aa5fcd7c29",
     "grade": true,
     "grade_id": "cell-e997f7a37037ef42",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalboostforest_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9f1877c2ea13bee7ac3b427cc5e9364",
     "grade": true,
     "grade_id": "cell-097ea4ff2d042e03",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs evalboostforest_test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b2ea8a51c328857abdf5f18e94709b8a",
     "grade": false,
     "grade_id": "cell-f827d27d11c4adaf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Two: Implement `GBRT` [Graded]\n",
    "\n",
    "Finally implement the function **`GBRT`**, which applies Gradient Boosting to the decision tree model. You can use the following pseudocode to write the function. You can also use your `evalboostforest` function to evaluate your boosted ensemble (provided you pass on the weights correctly).\n",
    "\n",
    "<center><img src=\"gbrt_pseudocode.png\" width=\"50%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0b7abc91388ed60171da5be1ec5ced8f",
     "grade": false,
     "grade_id": "cell-bgrt",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def GBRT(xTr, yTr, m, maxdepth=4, alpha=0.1):\n",
    "    \"\"\"\n",
    "    An implementation of Gradient Boosted Regression Trees with m trees of depth=maxdepth.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data points\n",
    "        yTr:      n-dimensional vector of labels\n",
    "        m:        number of trees in the forest\n",
    "        maxdepth: maximum depth of tree\n",
    "        alpha:    learning rate for the GBRT (also the weight for each tree in the ensemble)\n",
    "        \n",
    "        \n",
    "    Output:\n",
    "        trees, alphas\n",
    "        trees: list of decision trees of length m\n",
    "        alphas: weights of each tree\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = xTr.shape\n",
    "    trees = []\n",
    "    alphas = []\n",
    "    \n",
    "    # Ensemble tree H = 0 right now. Therefore, t_i = y_i for all i\n",
    "    # Make a copy of the ground truth label\n",
    "    # this will be the initial ground truth for the first GBRT\n",
    "    t = np.copy(yTr)\n",
    "    #Initialize t as a copy of yTr\n",
    "    #For each iteration:\n",
    "        #Create a tree, fit to xTr and t \n",
    "        #Append this tree to list of trees\n",
    "        #Append alpha to list of alphas\n",
    "        #Generate sum of weighted predictions of all trees with given alphas\n",
    "        #Calculate new t value by subtracting weighted predictions from yTr\n",
    "    #Return list of trees and list of alphas\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for i_m in range(m):\n",
    "        # create tee\n",
    "        tree = RegressionTree(depth=maxdepth)\n",
    "        # fit tree\n",
    "        tree.fit(xTr, t)\n",
    "        # append\n",
    "        trees.append(tree)\n",
    "        alphas.append(alpha)\n",
    "        # update t\n",
    "        pred_H = evalboostforest(trees, xTr, alphas)\n",
    "        t = yTr - pred_H\n",
    "        \n",
    "    return trees, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5cfacaa71b5e8f6ffc4db5fb82ece530",
     "grade": false,
     "grade_id": "cell-gbrt-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def GBRT_test1():\n",
    "    m = 40\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    trees, weights = GBRT(x, y, m, alpha=0.1)\n",
    "    return len(trees) == m and len(weights) == m # make sure there are m trees in the forest\n",
    "\n",
    "def GBRT_test2():\n",
    "    m = 20\n",
    "    x = np.arange(100).reshape((100, 1))\n",
    "    y = np.arange(100)\n",
    "    max_depth = 4\n",
    "    trees, weights = GBRT(x, y, m, max_depth)\n",
    "    depths_forest = np.array([tree.depth for tree in trees]) # Get the depth of all trees in the forest\n",
    "    return np.all(depths_forest == max_depth) # make sure that the max depth of all the trees is correct\n",
    "\n",
    "def GBRT_test3():\n",
    "    m = 4\n",
    "    xTrSpiral,yTrSpiral,_,_= spiraldata(150)\n",
    "    max_depth = 4\n",
    "    trees, weights = GBRT(xTrSpiral, yTrSpiral, m, max_depth, 1) # Create a gradient boosted forest with 4 trees\n",
    "    \n",
    "    errs = [] \n",
    "    for i in range(m):\n",
    "        predH = evalboostforest(trees[:i+1], xTrSpiral, weights[:i+1]) # calculate the prediction of the first i-th tree\n",
    "        err = np.mean(np.sign(predH) != yTrSpiral) # calculate the error of the first i-th tree\n",
    "        errs.append(err) # keep track of the error\n",
    "    \n",
    "    # your errs should be decreasing, i.e., the different between two subsequent errors is <= 0\n",
    "    return np.all(np.diff(errs) <= 0) \n",
    "    \n",
    "runtest(GBRT_test1, 'GBRT_test1')\n",
    "runtest(GBRT_test2, 'GBRT_test2')\n",
    "runtest(GBRT_test3, 'GBRT_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67c60f86562d63e8916f40d6cf7c86de",
     "grade": true,
     "grade_id": "cell-gbrt-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs GBRT_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "defaad0a75cd576658453751d462c813",
     "grade": true,
     "grade_id": "cell-gbrt-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs GBRT_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8675cc944004e0f62d7c72054135d80e",
     "grade": true,
     "grade_id": "cell-gbrt-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs GBRT_testt3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d2354c6b7c532cad228d52431cf1e8b",
     "grade": false,
     "grade_id": "cell-a84a7ba4a0e5d391",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize the Decision Boundary\n",
    "\n",
    "The following script visualizes the decision boundary of a boosted ensemble of CART trees. It should perform much better than just 1 CART tree of the same depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a86b1afd4d056d830f85d0c9f95c219",
     "grade": false,
     "grade_id": "cell-0b793c46d539ba5d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trees, weights=GBRT(xTrSpiral,yTrSpiral, 40, maxdepth=4, alpha=0.03) # compute tree on training data \n",
    "visclassifier(lambda X:evalboostforest(trees, X, weights),xTrSpiral,yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evalforest(trees,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evalforest(trees,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6985cf972c95de65e218e8e0571e0ae0",
     "grade": false,
     "grade_id": "cell-03a78b7b9ea09b9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Evaluate Test and Training Error</h3>\n",
    "\n",
    "<p>The following script evaluates the test and training error of an ensemble of CART trees as we vary the number of trees.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65a3d66cf5b17fbe6dceab5dfb000a9a",
     "grade": false,
     "grade_id": "cell-4c5550a5997aaf04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "M=40 # max number of trees\n",
    "err_trB=[]\n",
    "err_teB=[]\n",
    "alltrees, allweights =GBRT(xTrIon,yTrIon, M, maxdepth=4, alpha=0.05)\n",
    "for i in range(M):\n",
    "    trees=alltrees[:i+1]\n",
    "    weights=allweights[:i+1]\n",
    "    trErr = np.mean(np.sign(evalboostforest(trees,xTrIon, weights)) != yTrIon)\n",
    "    teErr = np.mean(np.sign(evalboostforest(trees,xTeIon, weights)) != yTeIon)\n",
    "    err_trB.append(trErr)\n",
    "    err_teB.append(teErr)\n",
    "    print(\"[%d]training err = %.4f\\ttesting err = %.4f\" % (i,trErr, teErr))\n",
    "\n",
    "plt.figure()\n",
    "line_tr, = plt.plot(range(M), err_trB, '-*', label=\"Training error\")\n",
    "line_te, = plt.plot(range(M), err_teB, '-*', label=\"Testing error\")\n",
    "plt.title(\"Gradient Boosted Trees\")\n",
    "plt.legend(handles=[line_tr, line_te])\n",
    "plt.xlabel(\"# of trees\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2D Interactive Demo on Spiral Data (adding trees with clicks)\n",
    "\n",
    "The following demo visualizes the GBRT classifier on the spiral data. With each click you can add one more regression tree to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,w,b,M,Q,trees,weights\n",
    "    \n",
    "    if event.key == 'shift': Q+=10\n",
    "    else: Q+=1\n",
    "    Q=min(Q,M)\n",
    "\n",
    "    classvals = np.unique(yTrain)\n",
    "        \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get forest\n",
    "    \n",
    "    fun = lambda X:evalboostforest(trees[:Q],X, weights[:Q])        \n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    trerr=np.mean(np.sign(fun(xTrain))==np.sign(yTrain))\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],xTrain[yTrain == c,1],marker=marker_symbols[idx],color='k')\n",
    "    plt.show()\n",
    "    plt.title('# Trees: %i Training Accuracy: %2.2f' % (Q,trerr))\n",
    "    \n",
    "        \n",
    "xTrain=xTrSpiral.copy()/14+0.5\n",
    "yTrain=yTrSpiral.copy()\n",
    "yTrain=yTrain.astype(int)\n",
    "\n",
    "# Hyper-parameters (feel free to play with them)\n",
    "M=50\n",
    "alpha=0.05;\n",
    "depth=5;\n",
    "trees, weights=GBRT(xTrain, yTrain, M,alpha=alpha,maxdepth=depth)\n",
    "Q=0;\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest) \n",
    "print('Click to add a tree.');\n",
    "plt.title('Click to start boosting on the spiral data.')\n",
    "visclassifier(lambda X: np.sum(X,1)*0,xTrain,yTrain,newfig=False)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1D Interactive Demo\n",
    "\n",
    "The next interactive demo shows a 1-dimensional curve fitted with GBRT. We sample 100 training data points from the curve with additive noise. Note how the training and testing errors decrease as you add trees to your ensemble, but the testing error eventually increases again. Initially (with only a few trees) you will see that the predicted (red) curve is very smooth and then becomes increasingly less smooth as you add more trees (here you are decreasing bias).\n",
    "\n",
    "In the end, the predicted curve becomes rather complicated and overfits to the training data (high variance). In this demo, you can nicely see how GBRT starts out with high bias (too smooth), then lowers the bias, but eventually ends up with high variance (very jiggery overfitting curve). As with hyperparameters in Machine Learning, the number of trees in GBRT has a significant impact on the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,Q,trees,weights\n",
    "    \n",
    "    if event.key == 'shift': Q+=10\n",
    "    else: Q+=1\n",
    "    Q=min(Q,M)\n",
    "\n",
    "\n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    pTest=evalboostforest(trees[:Q],xTest,weights[:Q])    \n",
    "    pTrain=evalboostforest(trees[:Q],xTrain,weights[:Q])    \n",
    "\n",
    "\n",
    "    errTrain=np.sqrt(np.mean((pTrain-yTrain)**2))\n",
    "    errTest=np.sqrt(np.mean((pTest-yTest)**2))\n",
    "\n",
    "    plt.plot(xTrain[:,0],yTrain,'bx')\n",
    "    plt.plot(xTest[:,0],pTest,'r-')\n",
    "    plt.plot(xTest[:,0],yTest,'k-')\n",
    "\n",
    "    plt.legend(['Training data','Prediction'])\n",
    "    plt.title('(%i Trees)  Error Tr: %2.4f, Te:%2.4f' % (Q,errTrain,errTest))\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "n=100;\n",
    "NOISE=0.05\n",
    "xTrain=np.array([np.linspace(0,1,n),np.zeros(n)]).T\n",
    "yTrain=2*np.sin(xTrain[:,0]*3)*(xTrain[:,0]**2)\n",
    "yTrain+=np.random.randn(yTrain.size)*NOISE;\n",
    "ntest=300;\n",
    "xTest=np.array([linspace(0,1,ntest),np.zeros(ntest)]).T\n",
    "yTest=2*np.sin(xTest[:,0]*3)*(xTest[:,0]**2)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Hyper-parameters (feel free to play with them)\n",
    "M=400\n",
    "alpha=0.05;\n",
    "depth=3;\n",
    "trees, weights=GBRT(xTrain, yTrain, M,alpha=alpha,maxdepth=depth)\n",
    "Q=0;\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest) \n",
    "print('Click to add a tree (shift-click and add 10 trees).');\n",
    "plt.title('Click to start boosting on this 1D data (Shift-click to add 10 trees).')\n",
    "plt.plot(xTrain[:,0],yTrain,'*')\n",
    "plt.plot(xTest[:,0],yTest,'k-')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2D Interactive Demo\n",
    "\n",
    "The following demo visualizes the GBRT classifier. Add your own points directly on the graph with click and shift+click to see the prediction boundaries. There will be a delay between clicks as the new classifier is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "beb0d161fe6aefb20e2256fbab60c048",
     "grade": false,
     "grade_id": "cell-614060398e2f62ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick_forest(event):\n",
    "    \"\"\"\n",
    "    Visualize forest, including new point\n",
    "    \"\"\"\n",
    "    global xTrain,yTrain,w,b,M\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTrain = np.concatenate((xTrain,pos), axis = 0)\n",
    "    yTrain = np.append(yTrain, label)\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(yTrain)\n",
    "        \n",
    "    w = np.array(w).flatten()\n",
    "    \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get forest\n",
    "    trees, weights=GBRT(xTrain, yTrain, M)\n",
    "    fun = lambda X:evalboostforest(trees,X, weights)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTrain[yTrain == c,0],\n",
    "            xTrain[yTrain == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "xTrain= np.array([[5,6]])\n",
    "b=yTrIon\n",
    "yTrain = np.array([1])\n",
    "w=xTrIon\n",
    "M=5\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_forest)\n",
    "print('Note: You may notice a delay when adding points to the visualization.')\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}