{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9df1a534f7875318fb0f4ee266be329",
     "grade": false,
     "grade_id": "cell-d89a9eaca8f89db2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement your own perceptron. You'll implement a linear classifier and the perceptron update functions. You'll also have a chance to visualize your perceptron.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board (found in the Live Labs section of this course) to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Save your notebook —</strong> Click <strong>Save and Checkpoint</strong> in the \"File\" menu.</li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2c70cff993cd2a9f66d1484486819e9",
     "grade": false,
     "grade_id": "cell-07165ae949d0e7dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b2bb3faccb2a89f1e510ce94c3e2dc0",
     "grade": false,
     "grade_id": "cell-347e83ddfe5848d2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "22f4fe23e260cdb971661254b956e542",
     "grade": false,
     "grade_id": "cell-9259529aa911a4e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>The Perceptron</h2>\n",
    "\n",
    "<p>The perceptron is a basic linear classifier. The following cells will walk you through steps and ask you to finish the necessary functions in a pre-defined order. Code cells requiring your input will display # YOUR CODE HERE and graded portions will be adequately labeled. Unless specified otherwise, do not use loops.<br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a7cf9f98d5546f228d3957ce23b860d2",
     "grade": false,
     "grade_id": "cell-30625fcf06df5461",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part One: Perceptron Update [Graded]\n",
    "\n",
    "Implement the function below to update the perceptron given an input vector, label, and weight vector. Do **not** check if an update is necessary. This function can assume that it is only called when an update should be performed. Hint: You simply need to implement $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "07d48c675b92ef204852201e5ca33ff8",
     "grade": false,
     "grade_id": "cell-perceptron_update",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_update(x,y,w):\n",
    "    \"\"\"\n",
    "    function w=perceptron_update(x,y,w);\n",
    "    \n",
    "    Updates the perceptron weight vector w using x and y\n",
    "    Input:\n",
    "    x : input vector of d dimensions (d)\n",
    "    y : corresponding label (-1 or +1)\n",
    "    w : weight vector of d dimensions\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector after updating (d)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # update weight vector\n",
    "    w_new = w + y * x\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "468bf73acb8734bd18bce6fc2c64fae6",
     "grade": false,
     "grade_id": "cell-perceptron_update_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This self test will check that your perceptron_update function returns the correct values for input vector [0,1], label -1, and weight vector [1,1]\n",
    "\n",
    "def test_perceptron_update1():\n",
    "    x = np.array([0,1])\n",
    "    y = -1\n",
    "    w = np.array([1,1])\n",
    "    w1 = perceptron_update(x,y,w)\n",
    "    return (w1.reshape(-1,) == np.array([1,0])).all()\n",
    "\n",
    "def test_perceptron_update2(): \n",
    "    x = np.random.rand(25)\n",
    "    y = 1\n",
    "    w = np.zeros(25)\n",
    "    w1 = perceptron_update(x,y,w)\n",
    "    return np.linalg.norm(w1-x)<1e-8\n",
    "\n",
    "\n",
    "def test_perceptron_update3():\n",
    "    x = np.random.rand(25)\n",
    "    y = -1\n",
    "    w = np.zeros(25)\n",
    "    w1 = perceptron_update(x,y,w)\n",
    "    return np.linalg.norm(w1+x)<1e-8\n",
    "\n",
    "\n",
    "runtest(test_perceptron_update1, 'test_perceptron_update1')\n",
    "runtest(test_perceptron_update2, 'test_perceptron_update2')\n",
    "runtest(test_perceptron_update3, 'test_perceptron_update3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d3f2d15afa08b39ecd2f5f6a3e051ad",
     "grade": true,
     "grade_id": "cell-test_perceptron_update1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_perceptron_update1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "569d029123485ef1e7af6574f736031c",
     "grade": true,
     "grade_id": "cell-test_perceptron_update2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_perceptron_update2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89cc496c04dd91a477b6e45451c55a64",
     "grade": true,
     "grade_id": "cell-test_perceptron_update3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_perceptron_update3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6411e7dc9d7822bcf7aabe2186e194e9",
     "grade": false,
     "grade_id": "cell-8c2c41b6b1bbec23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part Two: Implement Perceptron [Graded]\n",
    "\n",
    "Implement function **`perceptron`**. This should contain a loop that calls **`perceptron_update`** until it converges or the maximum iteration count, 100, has been reached. Make sure you randomize the order of the training data on each iteration (you can use [**`np.random.permutation()`**](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html) to do this.)\n",
    "\n",
    "We assume that the $d^{th}$ dimension of $\\mathbf{x}_i$ is 1, and so the last dimension of $\\mathbf{w}$ represents the bias term, which we implicitly absorbed into the weight vector. So you can implement the pseudocode directly.\n",
    "\n",
    "Pseudocode:\n",
    "\n",
    "<center><img src=\"perceptron_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e3d28703e75684e804001cff47cc46d9",
     "grade": false,
     "grade_id": "cell-perceptron",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perceptron(xs, ys):\n",
    "    \"\"\"\n",
    "    function w=perceptron(xs,ys);\n",
    "    \n",
    "    Returns the weight vector learned by the Perceptron classifier.\n",
    "    We assume that the last dimension of each xs is 1.\n",
    "    \n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd matrix)\n",
    "    ys : n labels (-1 or +1)\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector (d)\n",
    "    b : bias term\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = xs.shape     # so we have n input vectors, of d dimensions each\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "    # set some parameter for while loop\n",
    "    iter_max = 100\n",
    "    iter_counter = 0\n",
    "    # 1s to xs\n",
    "    xss = np.concatenate((xs, np.ones((n,1))), axis=1)\n",
    "    # b to w\n",
    "    ww = np.append(w, b)\n",
    "    \n",
    "    #start algorithm\n",
    "    while iter_counter < iter_max:\n",
    "        m = 0 # number of missclassifications\n",
    "        for i in np.random.permutation(n):\n",
    "            if ys[i] * np.dot(ww, xss[i]) <= 0:\n",
    "                # update weight vector and misclassification counter\n",
    "                ww = perceptron_update(xss[i], ys[i], ww)\n",
    "                m += 1\n",
    "        iter_counter += 1\n",
    "        print(f\"Iteration: {iter_counter}. Number of missclassifications: {m}.\")\n",
    "        if m == 0:\n",
    "            break \n",
    "        if iter_counter == iter_max:\n",
    "            print(f\"Maximum iterations reached.\")\n",
    "    w_star = ww[:-1]\n",
    "    b_star = ww[-1]\n",
    "    return w_star, b_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f8c525414e6a1d28c68711e8f80de4a0",
     "grade": false,
     "grade_id": "cell-perceptron_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# These self tests will check that your perceptron function successfully classifies points in two different linearly separable dataset \n",
    "\n",
    "def test_Perceptron1():\n",
    "    N = 100;\n",
    "    d = 10;\n",
    "    x = np.random.rand(N,d)\n",
    "    w = np.random.rand(1,d)\n",
    "    y = np.sign(w.dot(x.T))[0]\n",
    "    w, b = perceptron(x,y)\n",
    "    preds = classify_linear_grader(x,w,b)\n",
    "    return np.array_equal(preds.reshape(-1,),y.reshape(-1,))\n",
    "\n",
    "\n",
    "def test_Perceptron2():\n",
    "    x = np.array([ [-0.70072, -1.15826],  [-2.23769, -1.42917],  [-1.28357, -3.52909],  [-3.27927, -1.47949],  [-1.98508, -0.65195],  [-1.40251, -1.27096],  [-3.35145,-0.50274],  [-1.37491,-3.74950],  [-3.44509,-2.82399],  [-0.99489,-1.90591],   [0.63155,1.83584],   [2.41051,1.13768],  [-0.19401,0.62158],   [2.08617,4.41117],   [2.20720,1.24066],   [0.32384,3.39487],   [1.44111,1.48273],   [0.59591,0.87830],   [2.96363,3.00412],   [1.70080,1.80916]])\n",
    "    y = np.array([1]*10 + [-1]*10)\n",
    "    w, b =perceptron(x,y)\n",
    "    preds = classify_linear_grader(x,w,b)\n",
    "    return np.array_equal(preds.reshape(-1,),y.reshape(-1,))\n",
    "\n",
    "\n",
    "runtest(test_Perceptron1, 'test_Perceptron1')\n",
    "runtest(test_Perceptron2, 'test_Perceptron2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8644ba7f99d7c6552e3faf1b2c94b155",
     "grade": true,
     "grade_id": "cell-test_perceptron1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_Perceptron1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35a74a497edec71fae76750ee96c6206",
     "grade": true,
     "grade_id": "cell-test_perceptron2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_Perceptron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize Your Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e6d0858a8c9238329d377078880e7fdb",
     "grade": false,
     "grade_id": "cell-c427edd1e26be6d6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once you have **`perceptron`** implemented, we can visualize how it finds `w` that classify points in a 2-dimensional plane. In this example, we center the data by subtracting the common mean vector of $\\mathbf{x}_i$ from each of $\\mathbf{x}_i$. Consequently, the perceptron algorithm will learn the bias term (the last dimension of $\\mathbf{w}$ as 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc3fa5d29e1e16c12a38108a06782bc0",
     "grade": false,
     "grade_id": "cell-perceptron-2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_perceptron_2D(x, y_true, w_true, y_pred, w_pred, b):\n",
    "    w_pred = w_pred.flatten()\n",
    "    w_true = w_true.flatten()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    # Plot points\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y_true, cmap='viridis')\n",
    "\n",
    "    grid_lim = np.max(abs(x[:, :-1]))\n",
    "\n",
    "    def calc_x2(b, x1, w):\n",
    "        return -(b+w[0] * x1) / w[1]\n",
    "\n",
    "    # Make arrow for w_true: normalize by magnitude, then stretch to grid lim\n",
    "    w_true_mag = np.sqrt(np.sum(w_true[:-1] ** 2))\n",
    "    plt.quiver([0], [0], [(w_true[0] / w_true_mag) * grid_lim], [(w_true[1] / w_true_mag) * grid_lim],\n",
    "               scale=1, units='xy', color='k')\n",
    "    plt.annotate('w_true', xy=w_true[:-1] / w_true_mag * grid_lim + [.05, 0.], fontsize=12, color='k')\n",
    "\n",
    "    # Make decision line perpendicular to arrow\n",
    "    plt.plot([grid_lim, -grid_lim], [calc_x2(b, grid_lim, w_true), calc_x2(b, -grid_lim, w_true)],\n",
    "             color='k', label='true decision line')\n",
    "    \n",
    "    # Repeat for w_pred\n",
    "    w_pred_mag = np.sqrt(np.sum(w_pred[:-1] ** 2))\n",
    "    plt.quiver([0], [0], [(w_pred[0] / w_pred_mag) * grid_lim], [(w_pred[1] / w_pred_mag) * grid_lim],\n",
    "               scale=1, units='xy', color='r')\n",
    "    plt.annotate('w_pred', xy=w_pred[0] / w_pred_mag * grid_lim + [0., 0.1], fontsize=12, color='r')\n",
    "    plt.plot([grid_lim, -grid_lim], [calc_x2(b, grid_lim, w_pred), calc_x2(b, -grid_lim, w_pred)],\n",
    "             color='r', label='pred decision line')\n",
    "\n",
    "    plt.title(f'True and predicted decision line')\n",
    "    plt.xlim(-grid_lim, grid_lim)\n",
    "    plt.ylim(-grid_lim, grid_lim)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec97a86f72abedb169764c41d4e69d79",
     "grade": false,
     "grade_id": "cell-8c7244aa65717f45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N, d = 500, 2 # count of observations and features\n",
    "np.random.seed(1)\n",
    "x = np.random.rand(N, d)\n",
    "x -= np.mean(x) # by subtracting mean, we move the features to range [-0.5, 0.5]\n",
    "#x = np.hstack([ x, np.ones((N, 1)) ]) # Add constant dimension -- a column of 1s at the end\n",
    "\n",
    "w_true = np.random.rand(1,d) # generate w from which we compute labels \n",
    "w_true -= np.mean(w_true) # Only rotates the weight vector in 2d space\n",
    "\n",
    "y_true = np.sign(w_true.dot(x.T))[0]\n",
    "y_true[0] = -1 * y_true[0] # this makes a separation line impossible to compute and we can plot both w_true and w_pred distinctly\n",
    "\n",
    "w_pred, b = perceptron(x, y_true)\n",
    "y_pred = classify_linear_grader(x, w_pred, b).astype(int)\n",
    "\n",
    "plot_perceptron_2D(x, y_true, w_true, y_pred, w_pred, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77eb1a95c18e1b3e1fe10fe660a60f24",
     "grade": false,
     "grade_id": "cell-dfa3c34a4174dea1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# number of input vectors\n",
    "N = 100\n",
    "\n",
    "# generate random (linarly separable) data\n",
    "xs = np.random.rand(N, 2)*10-5\n",
    "\n",
    "# defining random hyperplane\n",
    "w0 = np.random.rand(2)\n",
    "b0 = np.random.rand()*2-1;\n",
    "\n",
    "# assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "ys = np.sign(xs.dot(w0)+b0)\n",
    "\n",
    "# call perceptron to find w from data\n",
    "w,b = perceptron(xs.copy(),ys.copy())\n",
    "\n",
    "# test if all points are classified correctly\n",
    "assert (all(np.sign(ys*(xs.dot(w)+b))==1.0))  # yw'x should be +1.0 for every input\n",
    "print(\"Looks like you passed the Perceptron test!\")\n",
    "\n",
    "# we can make a pretty visualization\n",
    "visboundary(w,b,xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Your perceptron model in action\n",
    "The following script generates a plot where you can manually add positive and negative values. Simply click on the plot space to add positive points and shift-click to add negative points. Once you have added at least one positive and one negative value each, the plot will generate a decision boundary from your perceptron model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "855731ed19c51c62068870f96701ae72",
     "grade": false,
     "grade_id": "cell-50afb80324faafd6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def onclick(event):\n",
    "    \"\"\"\n",
    "    Assume the last dimension of each of xs is 1. So the last dimension of w represents the bias term.\n",
    "    \"\"\"\n",
    "    global w,ldata,ax,line,xydata\n",
    "\n",
    "    pos=np.array([[event.xdata],[event.ydata], [1.]])\n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    ax.plot(pos[0],pos[1],color)\n",
    "    ldata.append(label);\n",
    "    xydata=np.vstack((xydata,pos.T))\n",
    "\n",
    "    # call Perceptron function\n",
    "    w, b=perceptron(xydata,np.array(ldata).flatten())\n",
    "\n",
    "    # draw decision boundary\n",
    "    q=-b/(w**2).sum() *w;\n",
    "    if line is None:\n",
    "        line, = ax.plot([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]],'b--')\n",
    "    else:\n",
    "        line.set_data([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]])\n",
    "    return  \n",
    "        \n",
    "xydata=np.random.rand(0,2+1) # last one for constant dimension\n",
    "ldata=[]\n",
    "w=np.zeros(2)\n",
    "b=np.zeros(1)\n",
    "line=None\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "plt.title('Use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4729dbaecda0b61d35058cedc049e1f",
     "grade": false,
     "grade_id": "cell-341e13b424f0e6bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Part Three: Make Predictions [Graded]</h3>\n",
    "\n",
    "<p>Implement <b><code>classify_linear</code></b> that applies the weight vector (with implicit bias term) to the input vector. Make sure that the predictions returned are either 1 or -1.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3fa3276c394e9d3b7d19a593682f07c3",
     "grade": false,
     "grade_id": "cell-classify_linear",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classify_linear(xs,w,b=None):\n",
    "    \"\"\"\n",
    "    function preds=classify_linear(xs,w,b)\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd) [could also be a single vector of d dimensions]\n",
    "    w : weight vector of dimensionality d\n",
    "    b : bias (scalar)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions (1xn)\n",
    "    \"\"\"    \n",
    "    w = w.flatten()    \n",
    "    predictions=np.zeros(xs.shape[0])\n",
    "    # YOUR CODE HERE\n",
    "    # calculate results\n",
    "    res = np.dot(w, xs.T)\n",
    "    # assign labels\n",
    "    predictions[res>0] = 1.0\n",
    "    predictions[res<=0] = -1.0\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33c99da1051a4ede567586916c2cb36f",
     "grade": false,
     "grade_id": "cell-classify_linear_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run this self-test to check that your linear classifier correctly classifies the points in a linearly separable dataset\n",
    "\n",
    "def test_linear1():\n",
    "    xs = np.random.rand(50000,20)-0.5 # draw random data \n",
    "    xs = np.hstack([ xs, np.ones((50000,1)) ])\n",
    "    w0 = np.random.rand(20+1)\n",
    "    w0[-1] = -0.1 # with bias -0.1\n",
    "    ys = classify_linear(xs,w0)\n",
    "    uniquepredictions=np.unique(ys) # check if predictions are only -1 or 1\n",
    "    return set(uniquepredictions)==set([-1,1])\n",
    "\n",
    "def test_linear2():\n",
    "    xs = np.random.rand(1000,2)-0.5 # draw random data \n",
    "    xs = np.hstack([ xs, np.ones((1000, 1)) ])\n",
    "    w0 = np.array([0.5,-0.3,-0.1]) # define a random hyperplane with bias -0.1\n",
    "    ys = np.sign(xs.dot(w0)) # assign labels according to this hyperplane (so you know it is linearly separable)\n",
    "    return (all(np.sign(ys*classify_linear(xs,w0))==1.0))  # the original hyperplane (w0,b0) should classify all correctly\n",
    "\n",
    "runtest(test_linear1, 'test_linear1')\n",
    "runtest(test_linear2, 'test_linear2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "284874269dcfa256d567b2394d01a331",
     "grade": true,
     "grade_id": "cell-test_linear1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_linear1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d814bb703a2120d0509f6ff75e28e76",
     "grade": true,
     "grade_id": "cell-test_linear2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell- worth 1 point\n",
    "# runs test_linear2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35e1c95a793ed9446d3ea623b7f33487",
     "grade": false,
     "grade_id": "cell-16d4825fc16d8a0b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Scikit-learn approach\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/index.html) (or `sklearn`) is a Python library with implementations for numerous machine learning and data science models. Scikit-learn provides a standard step-by-step training procedure across different machine learning models, simplifying model training and prediction. The typical process for selecting, training, and predicting with a model of your choice is as follows:\n",
    "\n",
    "\n",
    "1. Define the classifier `clf` using a call to the model class.\n",
    "2. Train `clf` using `clf.fit` on training data.\n",
    "3. Predict on test data using `clf.predict`, getting predictions as output.\n",
    "\n",
    "Scikit-learn provides a [perceptron model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html). We provide a brief demonstration of its usage below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9584f216d359edd0ded1a8c37d6d36fd",
     "grade": false,
     "grade_id": "cell-9760161549d7c66b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N, d = 500, 2\n",
    "x = np.random.rand(N, d)\n",
    "x -= np.mean(x)\n",
    "# by subtracting mean, we move the features to range [-0.5, 0.5]\n",
    "x = np.hstack([ x, np.ones((N, 1)) ]) # Add constant dimension -- a column of 1s at the end\n",
    "\n",
    "w_true = np.random.rand(d)\n",
    "w_true -= np.mean(w_true) # Only rotates the weight vector in 2d space\n",
    "w_true = np.append(w_true, [0.]) # Add bias term equal to 0\n",
    "\n",
    "y_true = classify_linear_grader(x, w_true).astype(int)\n",
    "# change label of a data point\n",
    "y_true[np.random.randint(N)] *= -1 # this way perceptron will never learn w_true, and we can plot both w_true and w_pred distinctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e6e175baa447917817a21204fe93bec",
     "grade": false,
     "grade_id": "cell-7d3fbf537f63a103",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron(max_iter=100, fit_intercept=True)\n",
    "clf.fit(x[:, :-1], y_true) # Train on all features except the constant feature that was manually added\n",
    "w_pred_sklearn = np.concatenate([clf.coef_.flatten(), clf.intercept_.flatten()])\n",
    "\n",
    "# classify_linear_grader expects x to have 1s as the last column and \n",
    "y_pred_sklearn = classify_linear_grader(x, w_pred_sklearn).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d406e79cb1389a4422d3b18330cef64a",
     "grade": false,
     "grade_id": "cell-32f6c9ecf4e612ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_perceptron_2D(x, y_true, w_true, y_pred_sklearn, w_pred_sklearn, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}